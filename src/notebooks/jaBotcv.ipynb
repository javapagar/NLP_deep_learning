{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd023cf5e3c52ba4582b557c38c6f7215ea95d270dcb8c0a4a03826a3be328d56af",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# JaBot: chat bot para presentar mi CV\n",
    "\n",
    "A continuación, describiré los pasos para la construcción de un chatBot cuya función será contestar preguntas sobre mi CV.\n",
    "\n",
    "Cargo las librerias que utilizo:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting es-core-news-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.0.0/es_core_news_sm-3.0.0-py3-none-any.whl (13.9 MB)\n",
      "2021-04-12 15:36:49.563764: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-04-12 15:36:49.564175: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\javier\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (7.1.2)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.0.0\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "source": [
    "import json\n",
    "import spacy"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "source": [
    "## Obtención de datos\n",
    "\n",
    "Los datos que voy a utilizar se han construido de forma manual. He reunido una serie de preguntas o frases que mi robot podría recibir, y las he clasificado en función de una intención, por ejemplo, la etiqueta'Greetings' reunirá todas las cuestiones relacionadas con saludos. También se reunirán un listado de respuestas para cada intención, que se elegirán de forma aleatoria (idea: utilizar un algoritmo de similitud, como coseno, jaccard) y el bot la utilizará para contestar. Para estructurar esta información utilizaré el formato Json.\n",
    "\n",
    "Cargo los datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_json(path):\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "data = cargar_json('../testFiles/intentsCV.json')\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "source": [
    "# limpieza de datos\n",
    "\n",
    "1. Limpiar simbolos de puntuación: sustituir por espacios o borrarlo\n",
    "2. lematizar\n",
    "3. Borrar stop words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "#tokenizacion\n",
    "def tokenizer(document):\n",
    "    return [token for token in nlp(document)]\n",
    "\n",
    "def clean_lemma(document,tokenize=True):\n",
    "    if tokenize:\n",
    "        return [remove_accented_chars(token.lemma_) for token in tokenizer(document) if token.is_alpha or token.is_digit]\n",
    "    else:\n",
    "        return ' '.join([remove_accented_chars(token.lemma_) for token in tokenizer(document) if token.is_alpha or token.is_digit])\n",
    "\n",
    "def tokenizer_list(document_list):\n",
    "    return [tokenizer(document) for document in document_list]\n",
    "\n",
    "def clean_lemma_list(document_list,tokenize=True):\n",
    "    return [clean_lemma(remove_accented_chars(document),tokenize) for document in document_list]\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['haber alguien ahi alli', 'hey']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sentences_list = clean_lemma_list(training_sentences,tokenize=False)\n",
    "sentences_list[:2]\n"
   ]
  },
  {
   "source": [
    "# Usar sklearn para convertir el texto a variables numéricas\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True,\n",
    "                    ngram_range=(1,3))\n",
    "\n",
    "X_train = cv.fit_transform(sentences_list)\n",
    "\n",
    "X_train.shape, len(training_labels)\n",
    "lbl_encoder = LabelEncoder()\n",
    "\n",
    "y_test = lbl_encoder.fit_transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_model(model,train,target):\n",
    "    mod = model\n",
    "\n",
    "    params = {'C':[0.01,0.05,0.25,0.5,1]}\n",
    "\n",
    "    grid = GridSearchCV(mod,params,cv=5)\n",
    "    grid.fit(train,target)\n",
    "\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Javier\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "best_svm= train_model(svm,X_train,y_test)\n",
    "\n",
    "import pickle\n",
    "with open('svm.pickle', 'wb') as rick:\n",
    "    pickle.dump(best_svm, rick, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User: ['skills']\n",
      "ChatBot: Sí, tengo bastante experiencia\n",
      "User: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "data = cargar_json('../testFiles/intentsCV.json')\n",
    "\n",
    "with open('svm.pickle', 'rb') as handle:\n",
    "    best_svm = pickle.load(handle)\n",
    "\n",
    "while True:\n",
    "    print(\"User: \", end=\"\")\n",
    "    inp = input()\n",
    "    if inp.lower() == \"quit\":\n",
    "        break\n",
    "    input_text =  clean_lemma_list([inp],tokenize=False)\n",
    "    #print(input_text)\n",
    "    inp_code=cv.transform(input_text)\n",
    "    resp = best_svm.predict(inp_code)\n",
    "    \n",
    "    tag = lbl_encoder.inverse_transform(resp)\n",
    "\n",
    "    print(tag)\n",
    "\n",
    "    for i in data['intents']:\n",
    "        if i['tag'] == tag:\n",
    "            print(\"ChatBot:\", np.random.choice(i['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}