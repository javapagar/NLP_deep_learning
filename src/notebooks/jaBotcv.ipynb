{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# JaBot: chat bot para presentar mi CV\n",
    "\n",
    "A continuación, describiré los pasos para la construcción de un chatBot cuya función será contestar preguntas sobre mi CV.\n",
    "\n",
    "Cargo las librerias que utilizo:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "source": [
    "import json\n",
    "import spacy"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "source": [
    "## Obtención de datos\n",
    "\n",
    "Los datos que voy a utilizar se han construido de forma manual. He reunido una serie de preguntas o frases que mi robot podría recibir, y las he clasificado en función de una intención, por ejemplo, la etiqueta'Greetings' reunirá todas las cuestiones relacionadas con saludos. También se reunirán un listado de respuestas para cada intención, que se elegirán de forma aleatoria (idea: utilizar un algoritmo de similitud, como coseno, jaccard) y el bot la utilizará para contestar. Para estructurar esta información utilizaré el formato Json.\n",
    "\n",
    "Cargo los datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_json(path):\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "data = cargar_json('../testFiles/intentsCV.json')\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "source": [
    "# limpieza de datos\n",
    "\n",
    "1. Limpiar simbolos de puntuación: sustituir por espacios o borrarlo\n",
    "2. lematizar\n",
    "3. Borrar stop words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "#tokenizacion\n",
    "def tokenizer(document):\n",
    "    return [token for token in nlp(document)]\n",
    "\n",
    "def clean_lemma(document,tokenize=True):\n",
    "    if tokenize:\n",
    "        return [remove_accented_chars(token.lemma_) for token in tokenizer(document) if token.is_alpha or token.is_digit]\n",
    "    else:\n",
    "        return ' '.join([remove_accented_chars(token.lemma_) for token in tokenizer(document) if token.is_alpha or token.is_digit])\n",
    "\n",
    "def tokenizer_list(document_list):\n",
    "    return [tokenizer(document) for document in document_list]\n",
    "\n",
    "def clean_lemma_list(document_list,tokenize=True):\n",
    "    return [clean_lemma(remove_accented_chars(document),tokenize) for document in document_list]\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['haber alguien ahi alli', 'hey']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "sentences_list = clean_lemma_list(training_sentences,tokenize=False)\n",
    "sentences_list[:2]\n"
   ]
  },
  {
   "source": [
    "# Usar sklearn para convertir el texto a variables numéricas\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True,\n",
    "                    ngram_range=(1,3))\n",
    "\n",
    "X_train = cv.fit_transform(sentences_list)\n",
    "\n",
    "X_train.shape, len(training_labels)\n",
    "lbl_encoder = LabelEncoder()\n",
    "\n",
    "y_test = lbl_encoder.fit_transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_model(model,train,target):\n",
    "    mod = model\n",
    "\n",
    "    params = {'C':[0.01,0.05,0.25,0.5,1]}\n",
    "\n",
    "    grid = GridSearchCV(mod,params,cv=5)\n",
    "    grid.fit(train,target)\n",
    "\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Javier\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "best_svm= train_model(svm,X_train,y_test)\n",
    "\n",
    "import pickle\n",
    "with open('svm.pickle', 'wb') as rick:\n",
    "    pickle.dump(best_svm, rick, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User: ['skills']\n",
      "ChatBot: Sí, tengo bastante experiencia\n",
      "User: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "data = cargar_json('../testFiles/intentsCV.json')\n",
    "\n",
    "with open('svm.pickle', 'rb') as handle:\n",
    "    best_svm = pickle.load(handle)\n",
    "\n",
    "while True:\n",
    "    print(\"User: \", end=\"\")\n",
    "    inp = input()\n",
    "    if inp.lower() == \"quit\":\n",
    "        break\n",
    "    input_text =  clean_lemma_list([inp],tokenize=False)\n",
    "    #print(input_text)\n",
    "    inp_code=cv.transform(input_text)\n",
    "    resp = best_svm.predict(inp_code)\n",
    "    \n",
    "    tag = lbl_encoder.inverse_transform(resp)\n",
    "\n",
    "    print(tag)\n",
    "\n",
    "    for i in data['intents']:\n",
    "        if i['tag'] == tag:\n",
    "            print(\"ChatBot:\", np.random.choice(i['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}