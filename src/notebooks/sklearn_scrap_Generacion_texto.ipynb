{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Manejo de datos scrapeados de Sklearn user guide\n",
    "\n",
    "Estudio de los datos scrapeados de la guia de usuario de Scikit-learn [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "Cargo los datos de un archivo pickle previamente descargado por un script de scrapping en Python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 649 entries, 0 to 648\nData columns (total 6 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   level0   649 non-null    object\n 1   level1   649 non-null    object\n 2   content  649 non-null    object\n 3   level2   592 non-null    object\n 4   level3   344 non-null    object\n 5   level4   58 non-null     object\ndtypes: object(6)\nmemory usage: 30.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('sklearn_guide.plk','rb') as rick:\n",
    "    df_guide= pickle.load(rick)\n",
    "df_guide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                level0                               level1  \\\n",
       "0  Supervised learning  Neural network models (supervised)¶   \n",
       "1  Supervised learning                       Linear Models¶   \n",
       "2  Supervised learning                       Linear Models¶   \n",
       "3  Supervised learning                       Linear Models¶   \n",
       "4  Supervised learning                       Linear Models¶   \n",
       "\n",
       "                                             content  \\\n",
       "0  The following are a set of methods intended fo...   \n",
       "1  It is possible to constrain all the coefficien...   \n",
       "2  The least squares solution is computed using t...   \n",
       "3  LinearRegression fits a linear model with coef...   \n",
       "4  Ridge regression addresses some of the problem...   \n",
       "\n",
       "                                 level2                              level3  \\\n",
       "0                                   NaN                                 NaN   \n",
       "1               Ordinary Least Squares¶         Non-Negative Least Squares¶   \n",
       "2               Ordinary Least Squares¶  Ordinary Least Squares Complexity¶   \n",
       "3               Ordinary Least Squares¶                                 NaN   \n",
       "4  Ridge regression and classification¶                         Regression¶   \n",
       "\n",
       "  level4  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3    NaN  \n",
       "4    NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>level0</th>\n      <th>level1</th>\n      <th>content</th>\n      <th>level2</th>\n      <th>level3</th>\n      <th>level4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Supervised learning</td>\n      <td>Neural network models (supervised)¶</td>\n      <td>The following are a set of methods intended fo...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Supervised learning</td>\n      <td>Linear Models¶</td>\n      <td>It is possible to constrain all the coefficien...</td>\n      <td>Ordinary Least Squares¶</td>\n      <td>Non-Negative Least Squares¶</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Supervised learning</td>\n      <td>Linear Models¶</td>\n      <td>The least squares solution is computed using t...</td>\n      <td>Ordinary Least Squares¶</td>\n      <td>Ordinary Least Squares Complexity¶</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Supervised learning</td>\n      <td>Linear Models¶</td>\n      <td>LinearRegression fits a linear model with coef...</td>\n      <td>Ordinary Least Squares¶</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Supervised learning</td>\n      <td>Linear Models¶</td>\n      <td>Ridge regression addresses some of the problem...</td>\n      <td>Ridge regression and classification¶</td>\n      <td>Regression¶</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df_guide.head()"
   ]
  },
  {
   "source": [
    "## Pretratamiento con spacy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "De momento no es necesario"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(649, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df_guide.shape"
   ]
  },
  {
   "source": [
    "Guardo todo en una variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list =[]\n",
    "\n",
    "for i,row in df_guide.iterrows():\n",
    "    #level0 = row.loc['level0'].replace('¶','')\n",
    "    #level1 = row.loc['level1'].replace('¶','')\n",
    "    #level2 = row.loc['level2'].replace('¶','')\n",
    "    #level3 = row.loc['level3'].replace('¶','')\n",
    "    #level4 = row.loc['level4'].replace('¶','')\n",
    "    content= row.loc['content'].replace('¶','')\n",
    "    text_list.append(content)\n",
    "  \n",
    "text = '. '.join(text_list)\n"
   ]
  },
  {
   "source": [
    "# tratamiento de texto con tensor flow y keras\n",
    "\n",
    "## Tokenización\n",
    "\n",
    "Voy a tokenizar el texto a nivel carácter:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[8, 2, 5, 19, 11, 20, 7, 9]]\n['n e i g h b o r']\n79 550102\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "word_vector =tokenizer.texts_to_sequences(['neighbor'])\n",
    "print(word_vector)\n",
    "vector_word=tokenizer.sequences_to_texts(word_vector)\n",
    "print(vector_word)\n",
    "\n",
    "\n",
    "nDstinctChar = len(tokenizer.word_index)\n",
    "nChars = tokenizer.document_count\n",
    "print(nDstinctChar,nChars)\n"
   ]
  },
  {
   "source": [
    "## dividir el conjunto de datos secuencialmente\n",
    "\n",
    "El conjunto de entrenamiento sera el 90%, 5% para el de validadción y 5% para test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "495091"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "[codificacion] =np.array(tokenizer.texts_to_sequences([text]))-1 # la codificación irá de 0 a 78\n",
    "train_size =nChars * 90 //100\n",
    "train_set = tf.data.Dataset.from_tensor_slices(codificacion[:train_size])\n",
    "len(train_set)"
   ]
  },
  {
   "source": [
    "train_set es un vector con casi 600000 elementos, para pasarlo a una red neuronal necesito dividirlo en pequeñas porciones de texto, de unos 100 caracteres por ejemplo. El método window permite realizar esto. Comenzará en a crear ventanas de 100 elementos desde la posición uno, pasando a la dos, tres..., creando un conjuto de vectores de 100 elementos de (600000 / 100) * 600000 aproximadamente.\n",
    "\n",
    "la ventana se configura con el tramaño, shift es el número de pasos que avanza la ventana cada vez y drop_remainder a True hará que el tamaño de la venatana no vaya disminuyendo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 100 + 1 # es el tamaño del subvector =100 mas el paso\n",
    "train_set=train_set.repeat().window(window_size,shift=1,drop_remainder=True)\n"
   ]
  },
  {
   "source": [
    "aplanamos el conjunto de datos con el tamaño de la ventana (101)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set =train_set.flat_map(lambda window : window.batch(window_size))"
   ]
  },
  {
   "source": [
    "Se realiza un mezclado de las ventanas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = train_set.shuffle(10000).batch(batch_size)\n",
    "train_set = train_set.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "source": [
    "Voy a realizar la codificación one-hot para crear la bolsa de palabras con los 79 caracteres distintos que se manejaban, y añado la precarga"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(lambda X, y:(tf.one_hot(X, depth=nDstinctChar),y))\n",
    "train_set=train_set.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 100, 79) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in train_set.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "source": [
    "## Creación de la red neuronal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "15471/15471 [==============================] - 5579s 360ms/step - loss: 1.6821\n",
      "Epoch 2/10\n",
      "15471/15471 [==============================] - 2444s 158ms/step - loss: 1.3279\n",
      "Epoch 3/10\n",
      "15471/15471 [==============================] - 2447s 158ms/step - loss: 1.2821\n",
      "Epoch 4/10\n",
      "15471/15471 [==============================] - 2456s 159ms/step - loss: 1.2589\n",
      "Epoch 5/10\n",
      "15471/15471 [==============================] - 2448s 158ms/step - loss: 1.2451\n",
      "Epoch 6/10\n",
      "15471/15471 [==============================] - 2448s 158ms/step - loss: 1.2362\n",
      "Epoch 7/10\n",
      "15471/15471 [==============================] - 2446s 158ms/step - loss: 1.2280\n",
      "Epoch 8/10\n",
      "15471/15471 [==============================] - 2444s 158ms/step - loss: 1.2218\n",
      "Epoch 9/10\n",
      "15471/15471 [==============================] - 2446s 158ms/step - loss: 1.2177\n",
      "Epoch 10/10\n",
      "15471/15471 [==============================] - 2447s 158ms/step - loss: 1.2135\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, nDstinctChar],\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(nDstinctChar,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "source": [
    "## Preprocesamiento entrada\n",
    "\n",
    "Para probar el modelo voy a crear unas funciones auxiliares que realicen el posprocesamiento: la tokenización y la codificación one-hot.\n",
    "Creo una función para predecir el siguiente caracter y otra para que cree un bucle y genere texto"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'support vector machines (e.g. the coefficients \\\\(\\\\ell_2\\\\) is the coefficients \\\\(\\\\ell_2\\\\) is th'"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "\n",
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1\n",
    "    return tf.one_hot(input_token,nDstinctChar)\n",
    "\n",
    "def next_char(input):\n",
    "    aux = treament([input])\n",
    "    predic = np.argmax(model(aux),axis=-1)\n",
    "    #predic = model(input)[0,-1:,:].numpy() +1\n",
    "    return tokenizer.sequences_to_texts(predic+1)[0][-1]\n",
    "\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=80, temperature=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text)\n",
    "    return text\n",
    "\n",
    "input = 'support vector'\n",
    "complete_text(input)"
   ]
  },
  {
   "source": [
    "Parece que el modelo, a la hora de predecir, se queda enganchado y repite la misma frase una y otra vez, es más pong lo que pongo siempre converge en el mismo discurso. Necesita introducir algo de aletoriedad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Linear model that computes the above similarity is the\\ncoefficients \\\\(\\\\ell_0\\\\) is the model '"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "def next_char(input):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / 0.8\n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "input = 'Linear model'\n",
    "complete_text(input)"
   ]
  },
  {
   "source": [
    "## Guardar el modelo de generciónde texto por caracteres"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializar el modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"char_model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serializar los pesos a HDF5\n",
    "model.save_weights(\"char_model/model.h5\")\n"
   ]
  },
  {
   "source": [
    "Mejorando el modelo anterior a través del estado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "495091"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "#Creo el dataset de train con el 90%\n",
    "train_ds_estado = tf.data.Dataset.from_tensor_slices(codificacion[:nChars * 90 // 100])\n",
    "ds_size=len(train_ds_estado)\n",
    "#creo las ventanas, esta vez no hay ni solapamiento, el númreo de ventanas se reduce\n",
    "#tampoco hay mezcla, ya que las ventanas tienen que ser secuenciales donde acaba 1 empieza otra\n",
    "train_ds_estado = train_ds_estado.window(window_size,shift =100, drop_remainder=True)\n",
    "train_ds_estado=train_ds_estado.flat_map(lambda w: w.batch(window_size))\n",
    "#los lotes serán equivalentes a la ventana, de esta manera se respetará la secuencialida entre lotes\n",
    "train_ds_estado =train_ds_estado.batch(1)\n",
    "train_ds_estado = train_ds_estado.map(lambda w: (w[:,:-1],w[:,1:]))\n",
    "train_ds_estado =train_ds_estado.map(lambda X,y: (tf.one_hot(X, depth=nDstinctChar),y))\n",
    "train_ds_estado=train_ds_estado.prefetch(1)\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, batch_input_shape= [1, None,nDstinctChar]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(nDstinctChar, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4999 - accuracy: 0.5570\n",
      "Epoch 2/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4988 - accuracy: 0.5576\n",
      "Epoch 3/50\n",
      "4950/4950 [==============================] - 233s 47ms/step - loss: 1.4989 - accuracy: 0.5578\n",
      "Epoch 4/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4967 - accuracy: 0.5582\n",
      "Epoch 5/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4967 - accuracy: 0.5581\n",
      "Epoch 6/50\n",
      "4950/4950 [==============================] - 233s 47ms/step - loss: 1.4977 - accuracy: 0.5577\n",
      "Epoch 7/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4932 - accuracy: 0.5595\n",
      "Epoch 8/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4927 - accuracy: 0.5590\n",
      "Epoch 9/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4929 - accuracy: 0.5592\n",
      "Epoch 10/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4922 - accuracy: 0.5595\n",
      "Epoch 11/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4922 - accuracy: 0.5601\n",
      "Epoch 12/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4904 - accuracy: 0.5609\n",
      "Epoch 13/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4909 - accuracy: 0.5598\n",
      "Epoch 14/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4930 - accuracy: 0.5589\n",
      "Epoch 15/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4920 - accuracy: 0.5591\n",
      "Epoch 16/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4866 - accuracy: 0.5611\n",
      "Epoch 17/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4877 - accuracy: 0.5607\n",
      "Epoch 18/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4840 - accuracy: 0.5622\n",
      "Epoch 19/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4889 - accuracy: 0.5607\n",
      "Epoch 20/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4879 - accuracy: 0.5601\n",
      "Epoch 21/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4863 - accuracy: 0.5614\n",
      "Epoch 22/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4847 - accuracy: 0.5612\n",
      "Epoch 23/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4870 - accuracy: 0.5616\n",
      "Epoch 24/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4865 - accuracy: 0.5612\n",
      "Epoch 25/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4851 - accuracy: 0.5611\n",
      "Epoch 26/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4853 - accuracy: 0.5618\n",
      "Epoch 27/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4833 - accuracy: 0.5620\n",
      "Epoch 28/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4817 - accuracy: 0.5619\n",
      "Epoch 29/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4821 - accuracy: 0.5625\n",
      "Epoch 30/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4833 - accuracy: 0.5621\n",
      "Epoch 31/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4791 - accuracy: 0.5626\n",
      "Epoch 32/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4835 - accuracy: 0.5614\n",
      "Epoch 33/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4800 - accuracy: 0.5624\n",
      "Epoch 34/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4789 - accuracy: 0.5630\n",
      "Epoch 35/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4798 - accuracy: 0.5628\n",
      "Epoch 36/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4786 - accuracy: 0.5633\n",
      "Epoch 37/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4770 - accuracy: 0.5637\n",
      "Epoch 38/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4792 - accuracy: 0.5626\n",
      "Epoch 39/50\n",
      "4950/4950 [==============================] - 240s 48ms/step - loss: 1.4789 - accuracy: 0.5631\n",
      "Epoch 40/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4764 - accuracy: 0.5634\n",
      "Epoch 41/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4786 - accuracy: 0.5626\n",
      "Epoch 42/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4796 - accuracy: 0.5625\n",
      "Epoch 43/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4747 - accuracy: 0.5645\n",
      "Epoch 44/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4726 - accuracy: 0.5647\n",
      "Epoch 45/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4740 - accuracy: 0.5644\n",
      "Epoch 46/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4718 - accuracy: 0.5648\n",
      "Epoch 47/50\n",
      "4950/4950 [==============================] - 240s 49ms/step - loss: 1.4703 - accuracy: 0.5654\n",
      "Epoch 48/50\n",
      "4950/4950 [==============================] - 240s 48ms/step - loss: 1.4752 - accuracy: 0.5641\n",
      "Epoch 49/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4732 - accuracy: 0.5640\n",
      "Epoch 50/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4738 - accuracy: 0.5644\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = ds_size // 100\n",
    "history = model.fit(train_ds_estado, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "source": [
    "El poblema del modelo anterior es que sólo permite hacer predicciones para lotes del mismo tamaño que el entrenado, por lo que la entrada debes de ser de 100 carácteres. Para poder hacer que la entrada tenga un tamaño sin determinar hay que crear una red neuranoal sin estado igual, y copiar los pesos del anterior modelo con estado entrenado a este nuevo modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_statless = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, nDstinctChar]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(nDstinctChar,activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "source": [
    "Para preparar el anterior modelo para que puedad guardar los pesos del modelo con estado habrá que especificar la estructura del tensor utilizado antes, que era la siguiente (\\[1, None,nDstinctChar\\]). Para permitir una entrada con cualquier tamaño:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_statless.build(tf.TensorShape([None,None,nDstinctChar]))\n",
    "base_model_statless.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprovecahndo el código anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'support vectors, results. with dimensionality requires a list. the cost of dorical feature ind'"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1\n",
    "    return tf.one_hot(input_token,nDstinctChar)\n",
    "\n",
    "def next_char(input,model):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / 1\n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "def complete_text(text, model,n_chars=80, temperature=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text,model)\n",
    "    return text\n",
    "\n",
    "input = 'line'\n",
    "complete_text(input, base_model_statless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializar el modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"char_model_state/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serializar los pesos a HDF5\n",
    "model.save_weights(\"char_model_state/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}