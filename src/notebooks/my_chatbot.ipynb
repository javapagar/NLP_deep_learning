{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Construcción de un chatBot\n",
    "\n",
    "## Preprocesamiento de la información\n",
    "\n",
    "Para preprocesar el texto voy a utilizar la libreria construida para NLP spaCy. La información que voya a tartar se recoge de una base de datos nutrida utilizando la API REST de Stack Overflow, que recoge preguntas y respuestas sobre Python y Machine learning.\n",
    "\n",
    "Compruebo la conexión a la bd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "source": [
    "Accedo a la base de datos para tratar la información a un formato que sirva para entrenar una red neuronal. Para tratar el texto se tokeniza por palabras, quedándome unicamente con las caracteres alphanuméricos, lo que dejará fuera espacios en blanco y símbolos de puntuación, reduciendo el ruido. También pasaré los tokens a minúsculas, y crearé otra aproximación a través de los lemas de los tokens para reducir la dimensionalidad. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../../db/Stackoverflow.db')\n",
    "cursor = con.cursor()\n",
    "rows = cursor.execute(\"Select * from questions\").fetchall()\n",
    "questions = []\n",
    "answers = []\n",
    "tags =[]\n",
    "vocabulary = []\n",
    "vocabulary_lemma=[]\n",
    "docs =[]\n",
    "docs_lemma =[]\n",
    "classes = []\n",
    "ids=[]\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    id = row[0]\n",
    "    text = row[1] + row[2]\n",
    "    doc = nlp(text)#uno pregunta y comentario\n",
    "    question = []\n",
    "    question_lemma = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha or token.is_digit:#limpio simbolos de puntuación, me quedo con caracteres y números\n",
    "            \n",
    "            question.append(token.text.lower())\n",
    "            question_lemma.append(token.lemma_.lower())\n",
    "    vocabulary.append(' '.join(question))\n",
    "    vocabulary_lemma.append(' '.join(question_lemma))\n",
    "\n",
    "    questions.append(text)\n",
    "    answers.append(row[4])\n",
    "    tagsRows = cursor.execute(\"Select t.tag from questions_tags as qt inner join tags as t on qt.id_tag=t.id where qt.id_question=?\",(id,)).fetchall()\n",
    "    tagList = []\n",
    "    for tag in tagsRows:\n",
    "        tagList.append(tag[0])\n",
    "    classes.extend(tagList)\n",
    "    tagList=sorted(tagList)\n",
    "    tags.append(';'.join(tagList))\n",
    "    docs.append((question,tagList))\n",
    "    docs_lemma.append((question_lemma,tagList))\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "source": [
    "Creo el vocabulario con las palabras utilizadas en los títulos de las preguntas. Pruebo dos formas, una con la propia palabra y otra con los lemas de las palabras. Vemos que partiendo de los lemas se reduce la dimensionalidad del vector vocabulario."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('how do you make this code more you guys please tell me how i can make the following code more pythonic the code is correct full disclosure it problem in handout 4 of this machine learning course i supposed to use newton algorithm on the two data sets for fitting a logistic hypothesis but they use matlab i using scipy eg one question i have is the matrixes kept rounding to integers until i initialized one value to is there a better way thanks',\n",
       " 'machine-learning;python;scipy')"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "import pickle\n",
    "with open('vocabulary_full.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(vocabulary, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('vocabulary_lemma_full.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocabulary_lemma, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "vocabulary[1], tags[1]"
   ]
  },
  {
   "source": [
    "Lo mismo hago para construir las clases. Aquí utilizo dos aproximaciones, una por concatenado alfabético de tags y otra con los tags por separado. La reducción es considerable si obtengo los términos de forma independiente, diferenciando 967 clases diferentes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3192\n967\n"
     ]
    }
   ],
   "source": [
    "# to save the fitted label\n",
    "with open('tags_full.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(tags, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('classes_full.pickle', 'wb') as handle:\n",
    "    pickle.dump(classes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "classes = list(sorted(set(classes)))\n",
    "tags =list(sorted(set(tags)))\n",
    "print(len(tags))\n",
    "print(len(classes))\n",
    "\n",
    "# to save the fitted label\n",
    "with open('tags.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(tags, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('classes.pickle', 'wb') as handle:\n",
    "    pickle.dump(classes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "source": [
    "Creo un CountVectorizer para crear el vector que codifique la pregunta, conocido como bags of words. Se le pasa el conjunto de tokens de la pregunta, que tendrá que estar tratado de la misma forma que el vocabulario, que se pasará como segundo parámetro."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x694108 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 141 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cv = CountVectorizer(binary = True,\n",
    "                    ngram_range=(1,3))\n",
    "X_train = cv.fit_transform(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((6839, 694108), 6839)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "with open('tags_full.pickle', 'rb') as handle:\n",
    "    y_tags = pickle.load(handle)\n",
    "\n",
    "X_train.shape, len(y_tags)"
   ]
  },
  {
   "source": [
    "Creo una función para crear el vector que codifique la pregunta, conocido como bags of words. Se le pasa el conjunto de tokens de la pregunta, que tendrá que estar tratado de la misma forma que el vocabulario, que se pasará como segundo parámetro."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lbl_encoder = LabelEncoder()\n",
    "\n",
    "y_test = lbl_encoder.fit_transform(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3968"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "import numpy as np\n",
    "def getBagWords(tokens,vocabulary):\n",
    "    bag = []\n",
    "    for word in vocabulary:\n",
    "        if word in tokens:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "bag = getBagWords(docs_lemma[7][0],vocabulary_lemma)\n",
    "len(bag)"
   ]
  },
  {
   "source": [
    "## Preprocesamineto con keras\n",
    "\n",
    "Keras proporciona funcionalidad para realizar todo el tratamiento anterior de una forma más sencilla.\n",
    "A pesar de esta facilidad, keras no posee ni lematización ni steaming, por lo que habrá que realizarlo con otra libreria\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n5\n{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\ndefaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n[[0.  0.  0.5 0.5 0.  0.  0.  0.  0. ]\n [0.  0.5 0.  0.  0.5 0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.5 0.5 0.  0. ]\n [0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# definir 5 documentos\n",
    "docs = ['Well done!',\n",
    "   'Good work',\n",
    "   'Great effort',\n",
    "   'nice work',\n",
    "   'Excellent!']\n",
    "# crear el tokenizador\n",
    "t = Tokenizer()\n",
    "# ajustar el tokenizador en los documentos\n",
    "t.fit_on_texts(questions)\n",
    "# resumir lo que se aprendió\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# enteros codificando documentos. el mode puede ser igual a\n",
    "#binary: Si cada palabra está presente o no en el documento. Este es el valor por defecto.\n",
    "#count: El recuento de cada palabra del documento.\n",
    "#tfidf: La puntuación de Frecuencia de documento inversa de texto (TF-IDF) para cada palabra del documento.\n",
    "#freq: La frecuencia de cada palabra como una relación de palabras dentro de cada documento.\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='freq')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}