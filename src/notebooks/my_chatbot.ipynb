{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Construcción de un chatBot\n",
    "\n",
    "## Preprocesamiento de la información\n",
    "\n",
    "Para preprocesar el texto voy a utilizar la libreria construida para NLP spaCy. La información que voya a tartar se recoge de una base de datos nutrida utilizando la API REST de Stack Overflow, que recoge preguntas y respuestas sobre Python y Machine learning.\n",
    "\n",
    "Compruebo la conexión a la bd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../../db/Stackoverflow.db')\n",
    "cursor = con.cursor()\n",
    "rows = cursor.execute(\"Select * from questions\").fetchall()\n",
    "questions = []\n",
    "answers = []\n",
    "tags =[]\n",
    "vocabulary = []\n",
    "vocabulary_lemma=[]\n",
    "docs =[]\n",
    "docs_lemma =[]\n",
    "classes = []\n",
    "for row in rows:\n",
    "    id = row[0]\n",
    "    doc = nlp(row[1])\n",
    "    question = []\n",
    "    question_lemma = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha or token.is_digit:#limpio simbolos de puntuación, me quedo con caracteres y números\n",
    "            question.append(token.text.lower())\n",
    "            question_lemma.append(token.lemma_.lower())\n",
    "    vocabulary.extend(question)\n",
    "    vocabulary_lemma.extend(question_lemma)\n",
    "\n",
    "    questions.append(row[1])\n",
    "    answers.append(row[4])\n",
    "    tagsRows = cursor.execute(\"Select t.tag from questions_tags as qt inner join tags as t on qt.id_tag=t.id where qt.id_question=?\",(id,)).fetchall()\n",
    "    tagList = []\n",
    "    for tag in tagsRows:\n",
    "        tagList.append(tag[0])\n",
    "    classes.extend(tagList)\n",
    "    tagList=sorted(tagList)\n",
    "    tags.append(';'.join(tagList))\n",
    "    docs.append((question,tagList))\n",
    "    docs_lemma.append((question_lemma,tagList))\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4678\n3968\n"
     ]
    }
   ],
   "source": [
    "vocabulary=list(set(sorted(vocabulary)))\n",
    "vocabulary_lemma=list(set(sorted(vocabulary_lemma)))\n",
    "print(len(vocabulary))\n",
    "print(len(vocabulary_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3192\n967\n"
     ]
    }
   ],
   "source": [
    "classes = list(sorted(set(classes)))\n",
    "tags =list(sorted(set(tags)))\n",
    "print(len(tags))\n",
    "print(len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3968"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "import numpy as np\n",
    "def getBagWords(tokens,vocabulary):\n",
    "    bag = []\n",
    "    for word in vocabulary:\n",
    "        if word in tokens:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "bag = getBagWords(docs_lemma[7][0],vocabulary_lemma)\n",
    "len(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}