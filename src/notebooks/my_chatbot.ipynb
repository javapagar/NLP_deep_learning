{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Construcción de un chatBot\n",
    "\n",
    "## Preprocesamiento de la información\n",
    "\n",
    "Para preprocesar el texto voy a utilizar la libreria construida para NLP spaCy. La información que voya a tartar se recoge de una base de datos nutrida utilizando la API REST de Stack Overflow, que recoge preguntas y respuestas sobre Python y Machine learning.\n",
    "\n",
    "Compruebo la conexión a la bd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accedo a la base de datos para tratar la información a un formato que sirva para entrenar una red neuronal. Para tratar el texto se tokeniza por palabras, quedándome unicamente con las caracteres alphanuméricos, lo que dejará fuera espacios en blanco y símbolos de puntuación, reduciendo el ruido. También pasaré los tokens a minúsculas, y crearé otra aproximación a través de los lemas de los tokens para reducir la dimensionalidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../../db/Stackoverflow.db')\n",
    "cursor = con.cursor()\n",
    "rows = cursor.execute(\"Select * from questions\").fetchall()\n",
    "questions = []\n",
    "answers = []\n",
    "tags =[]\n",
    "vocabulary = []\n",
    "vocabulary_lemma=[]\n",
    "docs =[]\n",
    "docs_lemma =[]\n",
    "classes = []\n",
    "for row in rows:\n",
    "    id = row[0]\n",
    "    doc = nlp(row[1])\n",
    "    question = []\n",
    "    question_lemma = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha or token.is_digit:#limpio simbolos de puntuación, me quedo con caracteres y números\n",
    "            question.append(token.text.lower())\n",
    "            question_lemma.append(token.lemma_.lower())\n",
    "    vocabulary.extend(question)\n",
    "    vocabulary_lemma.extend(question_lemma)\n",
    "\n",
    "    questions.append(row[1])\n",
    "    answers.append(row[4])\n",
    "    tagsRows = cursor.execute(\"Select t.tag from questions_tags as qt inner join tags as t on qt.id_tag=t.id where qt.id_question=?\",(id,)).fetchall()\n",
    "    tagList = []\n",
    "    for tag in tagsRows:\n",
    "        tagList.append(tag[0])\n",
    "    classes.extend(tagList)\n",
    "    tagList=sorted(tagList)\n",
    "    tags.append(';'.join(tagList))\n",
    "    docs.append((question,tagList))\n",
    "    docs_lemma.append((question_lemma,tagList))\n",
    "con.close()\n"
   ]
  },
  {
   "source": [
    "Creo el vocabulario con las palabras utilizadas en los títulos de las preguntas. Pruebo dos formas, una con la propia palabra y otra con los lemas de las palabras. Vemos que partiendo de los lemas se reduce la dimensionalidad del vector vocabulario."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4678\n3968\n"
     ]
    }
   ],
   "source": [
    "vocabulary=list(set(sorted(vocabulary)))\n",
    "vocabulary_lemma=list(set(sorted(vocabulary_lemma)))\n",
    "print(len(vocabulary))\n",
    "print(len(vocabulary_lemma))"
   ]
  },
  {
   "source": [
    "Lo mismo hago para construir las clases. Aquí utilizo dos aproximaciones, una por concatenado alfabético de tags y otra con los tags  por separado. La reducción es considerable si obtengo los términos de forma independiente, diferenciando 967 clases diferentes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3192\n967\n"
     ]
    }
   ],
   "source": [
    "classes = list(sorted(set(classes)))\n",
    "tags =list(sorted(set(tags)))\n",
    "print(len(tags))\n",
    "print(len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creo una función para crear el vector que codifique la pregunta, conocido como bags of words. Se le pasa el conjunto de tokens de la pregunta, que tendrá que estar tratado de la misma forma que el vocabulario, que se pasará como segundo parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3968"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "import numpy as np\n",
    "def getBagWords(tokens,vocabulary):\n",
    "    bag = []\n",
    "    for word in vocabulary:\n",
    "        if word in tokens:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "bag = getBagWords(docs_lemma[7][0],vocabulary_lemma)\n",
    "len(bag)"
   ]
  },
  {
   "source": [
    "## Preprocesamineto con keras\n",
    "\n",
    "Keras proporciona funcionalidad para realizar todo el tratamiento anterior de una forma más sencilla.\n",
    "A pesar de esta facilidad, keras no posee ni lematización ni steaming, por lo que habrá que realizarlo con otra libreria\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n5\n{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\ndefaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n[[0.  0.  0.5 0.5 0.  0.  0.  0.  0. ]\n [0.  0.5 0.  0.  0.5 0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.5 0.5 0.  0. ]\n [0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# definir 5 documentos\n",
    "docs = ['Well done!',\n",
    "   'Good work',\n",
    "   'Great effort',\n",
    "   'nice work',\n",
    "   'Excellent!']\n",
    "# crear el tokenizador\n",
    "t = Tokenizer()\n",
    "# ajustar el tokenizador en los documentos\n",
    "t.fit_on_texts(questions)\n",
    "# resumir lo que se aprendió\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# enteros codificando documentos. el mode puede ser igual a\n",
    "#binary: Si cada palabra está presente o no en el documento. Este es el valor por defecto.\n",
    "#count: El recuento de cada palabra del documento.\n",
    "#tfidf: La puntuación de Frecuencia de documento inversa de texto (TF-IDF) para cada palabra del documento.\n",
    "#freq: La frecuencia de cada palabra como una relación de palabras dentro de cada documento.\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='freq')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}