{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd023cf5e3c52ba4582b557c38c6f7215ea95d270dcb8c0a4a03826a3be328d56af",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "source": [
    "Proyecto compiado de: https://towardsdatascience.com/how-to-build-your-own-chatbot-using-deep-learning-bb41f970e281"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "#keras preprocesamiento\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "source": [
    "Lee el json y crea los conjuntos de datos para entrenar el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../testFiles/intentsCV.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "75\n75\n14\n14\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Hola, qué tal estas?', 'Hola', 'Qué tal?'],\n",
       " ['Adiós', 'Pasa un buen día', 'Espero que hablemos pronto'],\n",
       " ['De nada', 'Un placer'],\n",
       " ['Me gustaría guiarte a través del CV de Javier Aparicio',\n",
       "  'Puedo hablarte de la experiendcia profesional de Javier Aparicio'],\n",
       " ['Mi nombre es Bot, Javi Bot',\n",
       "  'Me llamo Javi Bot',\n",
       "  'Puedes dirigirte a mi como Javi Bot'],\n",
       " ['Estoy formandome como Data Scientist',\n",
       "  'Estoy estudiando  Machine Leraning'],\n",
       " ['Me falta del TFG para obtener el grado de Ingeniería Informática y, actualmente, estoy haciendo un Bootcamp sobre Data Science'],\n",
       " ['Ultimamente he estado en ANCERT y anteriormente estuve 14 años en el CSIC'],\n",
       " ['En ANCERT hice labores de analista programador VBA para análisis de datos y Java para backend'],\n",
       " ['En el CSIC hice todo tipo de labores informáticas, aunque sobre todo programar, tanto en VBA y Java, para el análisis de datos científicos'],\n",
       " ['Sí, tengo bastante experiencia',\n",
       "  'Poseo gran conocimiento',\n",
       "  'Lo he usado en diferentes proyectos con buenos resultados'],\n",
       " ['Hasta el 18 de mayo no estaré disponible, me interesaría un puesto de Data Scientist, y me encanta desarrollar aplicaciones, no tendría ningún problema en incorporarme a un puesto acorde a esto'],\n",
       " ['You can just easily create a new account from our web site',\n",
       "  'Just go to our web site and follow the guidelines to create a new account'],\n",
       " ['Please provide us your complaint in order to assist you',\n",
       "  'Please mention your complaint, we will reach you and sorry for any inconvenience caused']]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(training_labels))\n",
    "print(len(labels))\n",
    "print(len(responses))\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,  7,  7,  7,  7,\n",
       "       13, 13, 13, 13, 13, 13, 13, 13,  0,  0,  0,  0,  0,  0,  9,  9,  9,\n",
       "        9,  9, 10, 10, 10, 12, 12, 12,  5,  5,  5,  1,  1,  1,  4,  4,  4,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  6,  6,  6,  6,  3,\n",
       "        3,  3,  3,  3,  2,  2,  2])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)\n",
    "len(training_labels)\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['greeting', 'greeting', 'greeting', 'greeting', 'greeting',\n",
       "       'greeting', 'greeting', 'greeting', 'greeting', 'greeting',\n",
       "       'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye',\n",
       "       'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks',\n",
       "       'thanks', 'thanks', 'thanks', 'about', 'about', 'about', 'about',\n",
       "       'about', 'about', 'name', 'name', 'name', 'name', 'name', 'now',\n",
       "       'now', 'now', 'studies', 'studies', 'studies', 'experience',\n",
       "       'experience', 'experience', 'ancert', 'ancert', 'ancert', 'csic',\n",
       "       'csic', 'csic', 'skills', 'skills', 'skills', 'skills', 'skills',\n",
       "       'skills', 'skills', 'skills', 'skills', 'skills', 'skills',\n",
       "       'skills', 'future', 'future', 'future', 'future', 'createaccount',\n",
       "       'createaccount', 'createaccount', 'createaccount', 'createaccount',\n",
       "       'complaint', 'complaint', 'complaint'], dtype='<U13')"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "lbl_encoder.inverse_transform(training_labels)"
   ]
  },
  {
   "source": [
    "Tokenización de frases textuales con Tokenizer de keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Hey',\n",
       " 'Is anyone there?',\n",
       " 'Hola',\n",
       " 'Eo',\n",
       " 'Hello',\n",
       " 'Yep!',\n",
       " 'Qué tal?',\n",
       " 'Buenos días',\n",
       " 'Buenas tardes',\n",
       " 'Javi Bot',\n",
       " 'Bye',\n",
       " 'See you later',\n",
       " 'Goodbye',\n",
       " 'Adiós',\n",
       " 'Hasta luego',\n",
       " 'Hasta siempre',\n",
       " 'Thanks',\n",
       " 'Thank you',\n",
       " \"That's helpful\",\n",
       " 'Thanks for the help',\n",
       " 'Gracias',\n",
       " 'Ok',\n",
       " 'Perfecto',\n",
       " 'Muy bien',\n",
       " 'Who are you?',\n",
       " 'What are you?',\n",
       " 'Who you are?',\n",
       " '¿En qué puedes ayudarme?',\n",
       " '¿qué haces?',\n",
       " '¿Para qué sirves?',\n",
       " 'what is your name',\n",
       " 'what should I call you',\n",
       " '¿Quién eres?',\n",
       " '¿Cómo te llamas?',\n",
       " 'whats your name?',\n",
       " '¿Qué haces actualmente?',\n",
       " '¿Dónde estás ahora?',\n",
       " '¿Dónde trabajas?',\n",
       " 'Formación',\n",
       " 'Qué has esudiado?',\n",
       " 'Tienes estudios superiores',\n",
       " '¿Qué has hecho?',\n",
       " '¿Dónde has trabajado?',\n",
       " '¿qué empresas?',\n",
       " 'ANCERT',\n",
       " 'Agencia Notarial de Certificación',\n",
       " 'último trabajo',\n",
       " 'CSIC',\n",
       " 'Consejo Superior de Investigación Científica',\n",
       " 'trabajo anterior',\n",
       " 'JAVA',\n",
       " 'Python',\n",
       " 'SQL',\n",
       " 'PHP',\n",
       " 'VBA',\n",
       " 'bases de datos',\n",
       " 'Mysql',\n",
       " 'Spring boot',\n",
       " 'JPA',\n",
       " 'programación',\n",
       " 'desarrollo de aplicaciones',\n",
       " 'visualización',\n",
       " 'qué te gustaría hacer en el futuro?',\n",
       " 'después que termines el bootcamp',\n",
       " 'trabajo ideal?',\n",
       " 'Te interesaría tabajar con nosotros',\n",
       " 'I need to create a new account',\n",
       " 'how to open a new account',\n",
       " 'I want to create an account',\n",
       " 'can you create an account for me',\n",
       " 'how to open a new account',\n",
       " 'have a complaint',\n",
       " 'I want to raise a complaint',\n",
       " 'there is a complaint about a service']"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "source": [
    "Tratamiento de los textos para alimentar la red neuronal, Sería interesante probar una lematización en la tokenización"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'you': 1, 'a': 2, 'qué': 3, 'to': 4, 'account': 5, '¿qué': 6, 'i': 7, 'de': 8, 'is': 9, 'are': 10, 'what': 11, 'te': 12, '¿dónde': 13, 'has': 14, 'trabajo': 15, 'create': 16, 'new': 17, 'complaint': 18, 'there': 19, 'hasta': 20, 'thanks': 21, 'for': 22, 'who': 23, 'haces': 24, 'your': 25, 'name': 26, 'el': 27, 'how': 28, 'open': 29, 'want': 30, 'an': 31, 'hi': 32, 'hey': 33, 'anyone': 34, 'hola': 35, 'eo': 36, 'hello': 37, 'yep': 38, 'tal': 39, 'buenos': 40, 'días': 41, 'buenas': 42, 'tardes': 43, 'javi': 44, 'bot': 45, 'bye': 46, 'see': 47, 'later': 48, 'goodbye': 49, 'adiós': 50, 'luego': 51, 'siempre': 52, 'thank': 53, \"that's\": 54, 'helpful': 55, 'the': 56, 'help': 57, 'gracias': 58, 'ok': 59, 'perfecto': 60, 'muy': 61, 'bien': 62, '¿en': 63, 'puedes': 64, 'ayudarme': 65, '¿para': 66, 'sirves': 67, 'should': 68, 'call': 69, '¿quién': 70, 'eres': 71, '¿cómo': 72, 'llamas': 73, 'whats': 74, 'actualmente': 75, 'estás': 76, 'ahora': 77, 'trabajas': 78, 'formación': 79, 'esudiado': 80, 'tienes': 81, 'estudios': 82, 'superiores': 83, 'hecho': 84, 'trabajado': 85, 'empresas': 86, 'ancert': 87, 'agencia': 88, 'notarial': 89, 'certificación': 90, 'último': 91, 'csic': 92, 'consejo': 93, 'superior': 94, 'investigación': 95, 'científica': 96, 'anterior': 97, 'java': 98, 'python': 99, 'sql': 100, 'php': 101, 'vba': 102, 'bases': 103, 'datos': 104, 'mysql': 105, 'spring': 106, 'boot': 107, 'jpa': 108, 'programación': 109, 'desarrollo': 110, 'aplicaciones': 111, 'visualización': 112, 'gustaría': 113, 'hacer': 114, 'en': 115, 'futuro': 116, 'después': 117, 'que': 118, 'termines': 119, 'bootcamp': 120, 'ideal': 121, 'interesaría': 122, 'tabajar': 123, 'con': 124, 'nosotros': 125, 'need': 126, 'can': 127, 'me': 128, 'have': 129, 'raise': 130, 'about': 131, 'service': 132}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000 #número de palabras del vocabulario\n",
    "embedding_dim = 16\n",
    "max_len = 20 #tamaño del vector que representará al documento\n",
    "oov_token = \"<OOV>\" #ayuda a identificar palabras que quedan fuera del vocabulario\n",
    "\n",
    "tokenizer = Tokenizer()#crea la bolsa de palabras\n",
    "tokenizer.fit_on_texts(training_sentences)#con las palabras de los documentos (oraciones)\n",
    "word_index = tokenizer.word_index\n",
    "#print(tokenizer.word_counts) #cuanto se repite una palabra\n",
    "#print(tokenizer.document_count) #número de docs (frases)\n",
    "print(tokenizer.word_index)#palabra, índice\n",
    "#print(tokenizer.word_docs)#palabra índice de documento\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) #transforma las frases a vectores de ínidces de palabras\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)# lleva las secuencias a vectores del mismo tamaño\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[32],\n",
       " [33],\n",
       " [9, 34, 19],\n",
       " [35],\n",
       " [36],\n",
       " [37],\n",
       " [38],\n",
       " [3, 39],\n",
       " [40, 41],\n",
       " [42, 43],\n",
       " [44, 45],\n",
       " [46],\n",
       " [47, 1, 48],\n",
       " [49],\n",
       " [50],\n",
       " [20, 51],\n",
       " [20, 52],\n",
       " [21],\n",
       " [53, 1],\n",
       " [54, 55],\n",
       " [21, 22, 56, 57],\n",
       " [58],\n",
       " [59],\n",
       " [60],\n",
       " [61, 62],\n",
       " [23, 10, 1],\n",
       " [11, 10, 1],\n",
       " [23, 1, 10],\n",
       " [63, 3, 64, 65],\n",
       " [6, 24],\n",
       " [66, 3, 67],\n",
       " [11, 9, 25, 26],\n",
       " [11, 68, 7, 69, 1],\n",
       " [70, 71],\n",
       " [72, 12, 73],\n",
       " [74, 25, 26],\n",
       " [6, 24, 75],\n",
       " [13, 76, 77],\n",
       " [13, 78],\n",
       " [79],\n",
       " [3, 14, 80],\n",
       " [81, 82, 83],\n",
       " [6, 14, 84],\n",
       " [13, 14, 85],\n",
       " [6, 86],\n",
       " [87],\n",
       " [88, 89, 8, 90],\n",
       " [91, 15],\n",
       " [92],\n",
       " [93, 94, 8, 95, 96],\n",
       " [15, 97],\n",
       " [98],\n",
       " [99],\n",
       " [100],\n",
       " [101],\n",
       " [102],\n",
       " [103, 8, 104],\n",
       " [105],\n",
       " [106, 107],\n",
       " [108],\n",
       " [109],\n",
       " [110, 8, 111],\n",
       " [112],\n",
       " [3, 12, 113, 114, 115, 27, 116],\n",
       " [117, 118, 119, 27, 120],\n",
       " [15, 121],\n",
       " [12, 122, 123, 124, 125],\n",
       " [7, 126, 4, 16, 2, 17, 5],\n",
       " [28, 4, 29, 2, 17, 5],\n",
       " [7, 30, 4, 16, 31, 5],\n",
       " [127, 1, 16, 31, 5, 22, 128],\n",
       " [28, 4, 29, 2, 17, 5],\n",
       " [129, 2, 18],\n",
       " [7, 30, 4, 130, 2, 18],\n",
       " [19, 9, 2, 18, 131, 2, 132]]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "source": [
    "Creación de la red neuronal: Secuentials"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 20, 16)            16000     \n_________________________________________________________________\nglobal_average_pooling1d_3 ( (None, 16)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_10 (Dense)             (None, 16)                272       \n_________________________________________________________________\ndense_11 (Dense)             (None, 14)                238       \n=================================================================\nTotal params: 16,782\nTrainable params: 16,782\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6275 - accuracy: 0.7773\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.7895\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6196 - accuracy: 0.8079\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.8614\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.8302\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.8223\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.8380\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5824 - accuracy: 0.8380\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.8184\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.8274\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.8001\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.8274\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.8430\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.8352\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.8196\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.8380\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.8419\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.8536\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.8235\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.8391\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.8313\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.8235\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.8419\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8380\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.8419\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.8759\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.8524\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.8603\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.8786\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.8669\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.8630\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8736\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4595 - accuracy: 0.8986\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.9143\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8897\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.9053\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.9483\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.9182\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.9354\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.9209\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.9131\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.9237\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.9354\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.9460\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3853 - accuracy: 0.9616\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.9343\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.9499\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4050 - accuracy: 0.9460\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.9655\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.9538\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.9421\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3825 - accuracy: 0.9460\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - accuracy: 0.9343\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.9644\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.9566\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.9499\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3528 - accuracy: 0.9538\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3528 - accuracy: 0.9421\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.9499\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.9499\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.9499\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.9538\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.9499\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.9616\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.9343\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.9382\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.9538\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.9421\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.9421\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3063 - accuracy: 0.9448\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.9683\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.9683\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.9527\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2881 - accuracy: 0.9527\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2910 - accuracy: 0.9644\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.9527\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.9566\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.9343\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.9616\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2785 - accuracy: 0.9722\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.9683\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.9527\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2547 - accuracy: 0.9761\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.9644\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.9644\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.9527\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.9527\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9644\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.9527\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.9448\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.9448\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9605\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9644\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2355 - accuracy: 0.9605\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.9644\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9605\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9566\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9644\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2129 - accuracy: 0.9566\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9527\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.9527\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.9683\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.9527\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9527\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9527\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9605\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9527\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1939 - accuracy: 0.9566\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9710\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9894\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9683\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9527\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9566\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9644\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9722\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1691 - accuracy: 0.9894\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1563 - accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1661 - accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1645 - accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1533 - accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1340 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1265 - accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1017 - accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0972 - accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0882 - accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0789 - accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500 #número de veces en que todos los datos de entrenamiento pasan por la red en el proceso. Tip: aumentar el número de epoch hasta que la accuracy de los datos de validación (no los de entrenamiento) empiece a decrecer\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "source": [
    "Salvo el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: chat_model/assets\n"
     ]
    }
   ],
   "source": [
    "# to save the trained model\n",
    "model.save(\"chat_model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('chat_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Javier\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.23.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "with open('label_encoder.pickle', 'rb') as enc:\n",
    "    lbl_encoder = pickle.load(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../testFiles/intentsCV.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User: [8]\n",
      "[[4.1531106e-03 3.2032635e-03 8.8032124e-08 2.6369065e-10 1.3542097e-08\n",
      "  1.9321197e-07 1.1241958e-09 1.0516482e-03 9.8770463e-01 1.1781568e-04\n",
      "  3.5863916e-08 2.7333850e-03 1.9191318e-06 1.0339305e-03]]\n",
      "['greeting']\n",
      "ChatBot: Qué tal?\n",
      "User: [8]\n",
      "[[4.1531106e-03 3.2032635e-03 8.8032124e-08 2.6369065e-10 1.3542097e-08\n",
      "  1.9321197e-07 1.1241958e-09 1.0516482e-03 9.8770463e-01 1.1781568e-04\n",
      "  3.5863916e-08 2.7333850e-03 1.9191318e-06 1.0339305e-03]]\n",
      "['greeting']\n",
      "ChatBot: Hola, qué tal estas?\n",
      "User: [9]\n",
      "[[6.3152584e-08 5.0635594e-03 7.4084935e-04 2.0944201e-04 7.0663952e-10\n",
      "  3.3083424e-13 1.6524351e-03 2.0295415e-12 1.3558827e-04 9.9071538e-01\n",
      "  4.9076581e-14 9.3682000e-11 8.1877958e-04 6.6386577e-04]]\n",
      "['name']\n",
      "ChatBot: Mi nombre es Bot, Javi Bot\n",
      "User: [9]\n",
      "[[5.6143184e-05 2.2183008e-01 5.3294911e-04 4.3928485e-05 1.8445227e-06\n",
      "  8.0253448e-09 2.5672668e-03 1.3207576e-08 6.6685290e-03 6.7137206e-01\n",
      "  1.2007340e-09 9.1648729e-07 2.0931048e-02 7.5995170e-02]]\n",
      "['name']\n",
      "ChatBot: Puedes dirigirte a mi como Javi Bot\n",
      "User: [11]\n",
      "[[2.2320528e-01 9.6047901e-02 1.2940501e-06 2.5316909e-09 1.6657558e-03\n",
      "  2.9712785e-03 1.2729408e-06 1.0743496e-02 1.2647726e-01 1.5263364e-04\n",
      "  4.7859203e-04 2.9839095e-01 1.5931068e-03 2.3827118e-01]]\n",
      "['skills']\n",
      "ChatBot: Sí, tengo bastante experiencia\n",
      "User: [7]\n",
      "[[1.7449450e-03 8.6396349e-06 5.4493934e-11 4.9307935e-15 1.5026562e-07\n",
      "  2.5450386e-04 2.5779262e-13 7.8923172e-01 7.4379478e-04 2.4367408e-09\n",
      "  1.9554907e-05 2.0789331e-01 1.0274669e-08 1.0343895e-04]]\n",
      "['goodbye']\n",
      "ChatBot: Adiós\n",
      "User: [0]\n",
      "[[9.8580867e-01 1.4588764e-03 7.2319757e-08 8.2398359e-12 9.8670344e-06\n",
      "  1.5793624e-06 9.8782991e-09 4.6813420e-05 5.9738681e-03 3.3098906e-06\n",
      "  6.5032538e-04 5.3690034e-03 6.4254476e-04 3.5048928e-05]]\n",
      "['about']\n",
      "ChatBot: Me gustaría guiarte a través del CV de Javier Aparicio\n",
      "User: [5]\n",
      "[[3.1707097e-02 4.6241183e-02 1.5077323e-05 5.9388636e-08 1.4224081e-01\n",
      "  5.1099169e-01 4.6871734e-05 9.5054496e-04 4.7484139e-04 1.1165528e-05\n",
      "  1.1104632e-01 9.2793003e-02 9.6293548e-03 5.3851988e-02]]\n",
      "['experience']\n",
      "ChatBot: Ultimamente he estado en ANCERT y anteriormente estuve 14 años en el CSIC\n",
      "User: [1]\n",
      "[[3.7330082e-03 5.9511393e-01 5.1493239e-02 1.0279831e-03 7.1951013e-06\n",
      "  7.3648152e-06 1.5269766e-03 4.9966361e-06 1.3643250e-01 1.7485248e-01\n",
      "  8.0733462e-06 1.2277901e-04 2.9166771e-02 6.5027629e-03]]\n",
      "['ancert']\n",
      "ChatBot: En ANCERT hice labores de analista programador VBA para análisis de datos y Java para backend\n",
      "User: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "while True:\n",
    "    print(\"User: \", end=\"\")\n",
    "    inp = input()\n",
    "    if inp.lower() == \"quit\":\n",
    "        break\n",
    "    result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
    "                                                truncating='post', maxlen=20))\n",
    "    print([np.argmax(result)])\n",
    "    tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "    print(result)\n",
    "    print(tag)\n",
    "\n",
    "    for i in data['intents']:\n",
    "        if i['tag'] == tag:\n",
    "            print(\"ChatBot:\", np.random.choice(i['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}