{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "source": [
    "Proyecto compiado de: https://towardsdatascience.com/how-to-build-your-own-chatbot-using-deep-learning-bb41f970e281"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "#keras preprocesamiento\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "source": [
    "Lee el json y crea los conjuntos de datos para entrenar el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../testFiles/intents.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33\n33\n8\n8\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Hello', 'Hi', 'Hi there'],\n",
       " ['See you later', 'Have a nice day', 'Bye! Come back again'],\n",
       " ['Happy to help!', 'Any time!', 'My pleasure', \"You're most welcome!\"],\n",
       " ['I.m Joana, your bot assistant', \"I'm Joana, an Artificial Intelligent bot\"],\n",
       " ['You can call me Joana.', \"I'm Joana!\", 'Just call me as Joana'],\n",
       " ['Tell me how can assist you',\n",
       "  'Tell me your problem to assist you',\n",
       "  'Yes Sure, How can I support you'],\n",
       " ['You can just easily create a new account from our web site',\n",
       "  'Just go to our web site and follow the guidelines to create a new account'],\n",
       " ['Please provide us your complaint in order to assist you',\n",
       "  'Please mention your complaint, we will reach you and sorry for any inconvenience caused']]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(training_labels))\n",
    "print(len(labels))\n",
    "print(len(responses))\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 3, 3, 3, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 5, 5, 5, 5,\n",
       "       5, 5, 5, 2, 2, 2, 2, 2, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)\n",
    "len(training_labels)\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['greeting', 'greeting', 'greeting', 'greeting', 'greeting',\n",
       "       'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks',\n",
       "       'thanks', 'about', 'about', 'about', 'name', 'name', 'name',\n",
       "       'help', 'help', 'help', 'help', 'help', 'help', 'help',\n",
       "       'createaccount', 'createaccount', 'createaccount', 'createaccount',\n",
       "       'createaccount', 'complaint', 'complaint', 'complaint'],\n",
       "      dtype='<U13')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "lbl_encoder.inverse_transform(training_labels)"
   ]
  },
  {
   "source": [
    "Tokenización de frases textuales con Tokenizer de keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Hey',\n",
       " 'Is anyone there?',\n",
       " 'Hello',\n",
       " 'Hay',\n",
       " 'Bye',\n",
       " 'See you later',\n",
       " 'Goodbye',\n",
       " 'Thanks',\n",
       " 'Thank you',\n",
       " \"That's helpful\",\n",
       " 'Thanks for the help',\n",
       " 'Who are you?',\n",
       " 'What are you?',\n",
       " 'Who you are?',\n",
       " 'what is your name',\n",
       " 'what should I call you',\n",
       " 'whats your name?',\n",
       " 'Could you help me?',\n",
       " 'give me a hand please',\n",
       " 'Can you help?',\n",
       " 'What can you do for me?',\n",
       " 'I need a support',\n",
       " 'I need a help',\n",
       " 'support me please',\n",
       " 'I need to create a new account',\n",
       " 'how to open a new account',\n",
       " 'I want to create an account',\n",
       " 'can you create an account for me',\n",
       " 'how to open a new account',\n",
       " 'have a complaint',\n",
       " 'I want to raise a complaint',\n",
       " 'there is a complaint about a service']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "source": [
    "Tratamiento de los textos para alimentar la red neuronal, Sería interesante probar una lematización en la tokenización"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<OOV>': 1, 'you': 2, 'a': 3, 'i': 4, 'me': 5, 'to': 6, 'account': 7, 'help': 8, 'what': 9, 'is': 10, 'for': 11, 'are': 12, 'can': 13, 'need': 14, 'create': 15, 'new': 16, 'complaint': 17, 'there': 18, 'thanks': 19, 'who': 20, 'your': 21, 'name': 22, 'please': 23, 'support': 24, 'how': 25, 'open': 26, 'want': 27, 'an': 28, 'hi': 29, 'hey': 30, 'anyone': 31, 'hello': 32, 'hay': 33, 'bye': 34, 'see': 35, 'later': 36, 'goodbye': 37, 'thank': 38, \"that's\": 39, 'helpful': 40, 'the': 41, 'should': 42, 'call': 43, 'whats': 44, 'could': 45, 'give': 46, 'hand': 47, 'do': 48, 'have': 49, 'raise': 50, 'about': 51, 'service': 52}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000 #número de palabras del vocabulario\n",
    "embedding_dim = 16\n",
    "max_len = 20 #tamaño del vector que representará al documento\n",
    "oov_token = \"<OOV>\" #ayuda a identificar palabras que quedan fuera del vocabulario\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)#crea la bolsa de palabras\n",
    "tokenizer.fit_on_texts(training_sentences)#con las palabras de los documentos (oraciones)\n",
    "word_index = tokenizer.word_index\n",
    "#print(tokenizer.word_counts) #cuanto se repite una palabra\n",
    "#print(tokenizer.document_count) #número de docs (frases)\n",
    "print(tokenizer.word_index)#palabra, índice\n",
    "#print(tokenizer.word_docs)#palabra índice de documento\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) #transforma las frases a vectores de ínidces de palabras\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)# lleva las secuencias a vectores del mismo tamaño\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[29],\n",
       " [30],\n",
       " [10, 31, 18],\n",
       " [32],\n",
       " [33],\n",
       " [34],\n",
       " [35, 2, 36],\n",
       " [37],\n",
       " [19],\n",
       " [38, 2],\n",
       " [39, 40],\n",
       " [19, 11, 41, 8],\n",
       " [20, 12, 2],\n",
       " [9, 12, 2],\n",
       " [20, 2, 12],\n",
       " [9, 10, 21, 22],\n",
       " [9, 42, 4, 43, 2],\n",
       " [44, 21, 22],\n",
       " [45, 2, 8, 5],\n",
       " [46, 5, 3, 47, 23],\n",
       " [13, 2, 8],\n",
       " [9, 13, 2, 48, 11, 5],\n",
       " [4, 14, 3, 24],\n",
       " [4, 14, 3, 8],\n",
       " [24, 5, 23],\n",
       " [4, 14, 6, 15, 3, 16, 7],\n",
       " [25, 6, 26, 3, 16, 7],\n",
       " [4, 27, 6, 15, 28, 7],\n",
       " [13, 2, 15, 28, 7, 11, 5],\n",
       " [25, 6, 26, 3, 16, 7],\n",
       " [49, 3, 17],\n",
       " [4, 27, 6, 50, 3, 17],\n",
       " [18, 10, 3, 17, 51, 3, 52]]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "source": [
    "Creación de la red neuronal: Secuentials"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 16)            16000     \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 16)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                272       \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_2 (Dense)              (None, 8)                 136       \n=================================================================\nTotal params: 16,680\nTrainable params: 16,680\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3132 - accuracy: 0.4593\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3110 - accuracy: 0.4593\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2965 - accuracy: 0.4593\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3076 - accuracy: 0.4489\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2915 - accuracy: 0.4899\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2848 - accuracy: 0.4899\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2879 - accuracy: 0.4795\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2662 - accuracy: 0.4593\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2710 - accuracy: 0.4593\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2823 - accuracy: 0.4489\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2784 - accuracy: 0.4795\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2609 - accuracy: 0.5205\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2692 - accuracy: 0.5101\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2633 - accuracy: 0.5101\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.2451 - accuracy: 0.5205\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2433 - accuracy: 0.5713\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2373 - accuracy: 0.5818\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2354 - accuracy: 0.5407\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2386 - accuracy: 0.5713\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2286 - accuracy: 0.5818\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2368 - accuracy: 0.5713\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2191 - accuracy: 0.6020\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2179 - accuracy: 0.6020\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2085 - accuracy: 0.6124\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2043 - accuracy: 0.5713\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2023 - accuracy: 0.5818\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2075 - accuracy: 0.5713\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1882 - accuracy: 0.5713\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1932 - accuracy: 0.5713\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1908 - accuracy: 0.6020\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1713 - accuracy: 0.6124\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1721 - accuracy: 0.6020\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1765 - accuracy: 0.6020\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1584 - accuracy: 0.6124\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1636 - accuracy: 0.6020\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1527 - accuracy: 0.6124\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1491 - accuracy: 0.6124\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1477 - accuracy: 0.6020\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1458 - accuracy: 0.6430\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1445 - accuracy: 0.6430\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1432 - accuracy: 0.6938\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1425 - accuracy: 0.7551\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1337 - accuracy: 0.7961\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1401 - accuracy: 0.7857\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1405 - accuracy: 0.7857\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1373 - accuracy: 0.7857\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1285 - accuracy: 0.7857\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1246 - accuracy: 0.7857\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1175 - accuracy: 0.7551\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0910 - accuracy: 0.7348\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0925 - accuracy: 0.6938\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0753 - accuracy: 0.7042\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0771 - accuracy: 0.6736\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0841 - accuracy: 0.6632\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0574 - accuracy: 0.6736\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0546 - accuracy: 0.6736\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0699 - accuracy: 0.6938\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0540 - accuracy: 0.6938\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0500 - accuracy: 0.7244\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0588 - accuracy: 0.7244\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0344 - accuracy: 0.7655\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0386 - accuracy: 0.7857\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0348 - accuracy: 0.7857\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0326 - accuracy: 0.7857\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 1.0218 - accuracy: 0.7961\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0196 - accuracy: 0.7961\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0131 - accuracy: 0.7961\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0190 - accuracy: 0.7857\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0140 - accuracy: 0.7857\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0187 - accuracy: 0.7857\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0016 - accuracy: 0.7857\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9879 - accuracy: 0.7961\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9989 - accuracy: 0.7857\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9880 - accuracy: 0.7551\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9844 - accuracy: 0.7655\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9801 - accuracy: 0.7857\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9690 - accuracy: 0.7961\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9808 - accuracy: 0.7857\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9705 - accuracy: 0.7857\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9609 - accuracy: 0.7961\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9588 - accuracy: 0.8267\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9602 - accuracy: 0.8163\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9641 - accuracy: 0.6938\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9506 - accuracy: 0.7042\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9394 - accuracy: 0.7042\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9432 - accuracy: 0.6938\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9414 - accuracy: 0.6632\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9439 - accuracy: 0.6632\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9300 - accuracy: 0.6632\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.9221 - accuracy: 0.6938\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9171 - accuracy: 0.6632\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9209 - accuracy: 0.6632\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9012 - accuracy: 0.6430\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9064 - accuracy: 0.6326\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9038 - accuracy: 0.6326\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8983 - accuracy: 0.6326\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8987 - accuracy: 0.6326\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8880 - accuracy: 0.6430\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8815 - accuracy: 0.6430\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8803 - accuracy: 0.6430\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8772 - accuracy: 0.6326\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8678 - accuracy: 0.6430\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8726 - accuracy: 0.6326\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8685 - accuracy: 0.6632\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8656 - accuracy: 0.6632\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8592 - accuracy: 0.6632\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8566 - accuracy: 0.6632\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8476 - accuracy: 0.6430\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8372 - accuracy: 0.7042\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8379 - accuracy: 0.7348\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8430 - accuracy: 0.7857\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8366 - accuracy: 0.7857\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8329 - accuracy: 0.7857\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8226 - accuracy: 0.7961\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8259 - accuracy: 0.8163\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8145 - accuracy: 0.7961\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8142 - accuracy: 0.7551\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8049 - accuracy: 0.7961\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8091 - accuracy: 0.8163\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8112 - accuracy: 0.8163\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8114 - accuracy: 0.7551\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8005 - accuracy: 0.7042\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7966 - accuracy: 0.7348\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7937 - accuracy: 0.7348\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7933 - accuracy: 0.7348\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7986 - accuracy: 0.7244\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7831 - accuracy: 0.8775\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7799 - accuracy: 0.8775\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7829 - accuracy: 0.8775\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7713 - accuracy: 0.8163\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7571 - accuracy: 0.8267\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7714 - accuracy: 0.8163\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7522 - accuracy: 0.7961\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7561 - accuracy: 0.7857\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7641 - accuracy: 0.8163\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7551 - accuracy: 0.8163\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7568 - accuracy: 0.8163\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7440 - accuracy: 0.8163\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7519 - accuracy: 0.7857\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7366 - accuracy: 0.7857\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7437 - accuracy: 0.7857\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7311 - accuracy: 0.7961\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7261 - accuracy: 0.7961\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7292 - accuracy: 0.8469\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7293 - accuracy: 0.8469\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7297 - accuracy: 0.8775\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7259 - accuracy: 0.9081\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7287 - accuracy: 0.9081\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7307 - accuracy: 0.9081\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7247 - accuracy: 0.9081\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7202 - accuracy: 0.9081\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7064 - accuracy: 0.9081\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6996 - accuracy: 0.9081\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.9186\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7015 - accuracy: 0.9081\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6851 - accuracy: 0.9186\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.9081\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.9081\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6813 - accuracy: 0.9081\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6783 - accuracy: 0.9081\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.7857\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6687 - accuracy: 0.7961\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.7857\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6690 - accuracy: 0.7961\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6820 - accuracy: 0.7857\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.7857\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6793 - accuracy: 0.8163\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.8469\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.8163\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.8163\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6496 - accuracy: 0.8267\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.8163\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6514 - accuracy: 0.7857\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.7857\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6423 - accuracy: 0.7857\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6333 - accuracy: 0.7961\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.7961\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.7857\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6214 - accuracy: 0.7961\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6297 - accuracy: 0.7857\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6168 - accuracy: 0.7961\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6235 - accuracy: 0.7857\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6211 - accuracy: 0.8163\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6192 - accuracy: 0.8163\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.6058 - accuracy: 0.8163\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6040 - accuracy: 0.8163\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.8163\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6065 - accuracy: 0.8163\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5984 - accuracy: 0.8267\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6042 - accuracy: 0.8163\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.8163\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5940 - accuracy: 0.9388\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5845 - accuracy: 0.9388\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5858 - accuracy: 0.9388\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5943 - accuracy: 0.9388\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5791 - accuracy: 0.9492\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5784 - accuracy: 0.9388\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5788 - accuracy: 0.9388\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.9694\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.9694\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5635 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 500 #número de veces en que todos los datos de entrenamiento pasan por la red en el proceso. Tip: aumentar el número de epoch hasta que la accuracy de los datos de validación (no los de entrenamiento) empiece a decrecer\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}