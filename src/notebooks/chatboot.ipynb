{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd023cf5e3c52ba4582b557c38c6f7215ea95d270dcb8c0a4a03826a3be328d56af",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (3.15.5)  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\Javier\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\Javier\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\Javier\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Javier\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\Javier\\anaconda3\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\javier\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=33f964a4805e70eef342295f850a9a986e53749bf5b5526889b655b72b15fd25\n",
      "  Stored in directory: c:\\users\\javier\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=78a8893d747499fbd93a0311708a79bd5910120951013ad962a4c8c6e76b4f8d\n",
      "  Stored in directory: c:\\users\\javier\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: opt-einsum, gast, grpcio, flatbuffers, markdown, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, absl-py, tensorboard-plugin-wit, tensorboard, termcolor, keras-preprocessing, tensorflow-estimator, google-pasta, wrapt, astunparse, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.3.3 google-auth-1.28.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "source": [
    "Proyecto compiado de: https://towardsdatascience.com/how-to-build-your-own-chatbot-using-deep-learning-bb41f970e281"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "#keras preprocesamiento\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "source": [
    "Lee el json y crea los conjuntos de datos para entrenar el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../testFiles/intentsCV.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "    \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        \n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "75\n75\n14\n14\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Hola, qué tal estas?', 'Hola', 'Qué tal?'],\n",
       " ['Adiós', 'Pasa un buen día', 'Espero que hablemos pronto'],\n",
       " ['De nada', 'Un placer'],\n",
       " ['Me gustaría guiarte a través del CV de Javier Aparicio',\n",
       "  'Puedo hablarte de la experiendcia profesional de Javier Aparicio'],\n",
       " ['Mi nombre es Bot, Javi Bot',\n",
       "  'Me llamo Javi Bot',\n",
       "  'Puedes dirigirte a mi como Javi Bot'],\n",
       " ['Estoy formandome como Data Scientist',\n",
       "  'Estoy estudiando  Machine Leraning'],\n",
       " ['Me falta del TFG para obtener el grado de Ingeniería Informática y, actualmente, estoy haciendo un Bootcamp sobre Data Science'],\n",
       " ['Ultimamente he estado en ANCERT y anteriormente estuve 14 años en el CSIC'],\n",
       " ['En ANCERT hice labores de analista programador VBA para análisis de datos y Java para backend'],\n",
       " ['En el CSIC hice todo tipo de labores informáticas, aunque sobre todo programar, tanto en VBA y Java, para el análisis de datos científicos'],\n",
       " ['Sí, tengo bastante experiencia',\n",
       "  'Poseo gran conocimiento',\n",
       "  'Lo he usado en diferentes proyectos con buenos resultados'],\n",
       " ['Hasta el 18 de mayo no estaré disponible, me interesaría un puesto de Data Scientist, y me encanta desarrollar aplicaciones, no tendría ningún problema en incorporarme a un puesto acorde a esto'],\n",
       " ['You can just easily create a new account from our web site',\n",
       "  'Just go to our web site and follow the guidelines to create a new account'],\n",
       " ['Please provide us your complaint in order to assist you',\n",
       "  'Please mention your complaint, we will reach you and sorry for any inconvenience caused']]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(training_labels))\n",
    "print(len(labels))\n",
    "print(len(responses))\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,  7,  7,  7,  7,\n",
       "       13, 13, 13, 13, 13, 13, 13, 13,  0,  0,  0,  0,  0,  0,  9,  9,  9,\n",
       "        9,  9, 10, 10, 10, 12, 12, 12,  5,  5,  5,  1,  1,  1,  4,  4,  4,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  6,  6,  6,  6,  3,\n",
       "        3,  3,  3,  3,  2,  2,  2])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)\n",
    "len(training_labels)\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['greeting', 'greeting', 'greeting', 'greeting', 'greeting',\n",
       "       'greeting', 'greeting', 'greeting', 'greeting', 'greeting',\n",
       "       'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye',\n",
       "       'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks',\n",
       "       'thanks', 'thanks', 'thanks', 'about', 'about', 'about', 'about',\n",
       "       'about', 'about', 'name', 'name', 'name', 'name', 'name', 'now',\n",
       "       'now', 'now', 'studies', 'studies', 'studies', 'experience',\n",
       "       'experience', 'experience', 'ancert', 'ancert', 'ancert', 'csic',\n",
       "       'csic', 'csic', 'skills', 'skills', 'skills', 'skills', 'skills',\n",
       "       'skills', 'skills', 'skills', 'skills', 'skills', 'skills',\n",
       "       'skills', 'future', 'future', 'future', 'future', 'createaccount',\n",
       "       'createaccount', 'createaccount', 'createaccount', 'createaccount',\n",
       "       'complaint', 'complaint', 'complaint'], dtype='<U13')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "lbl_encoder.inverse_transform(training_labels)"
   ]
  },
  {
   "source": [
    "Tokenización de frases textuales con Tokenizer de keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Hey',\n",
       " 'Is anyone there?',\n",
       " 'Hola',\n",
       " 'Eo',\n",
       " 'Hello',\n",
       " 'Yep!',\n",
       " 'Qué tal?',\n",
       " 'Buenos días',\n",
       " 'Buenas tardes',\n",
       " 'Javi Bot',\n",
       " 'Bye',\n",
       " 'See you later',\n",
       " 'Goodbye',\n",
       " 'Adiós',\n",
       " 'Hasta luego',\n",
       " 'Hasta siempre',\n",
       " 'Thanks',\n",
       " 'Thank you',\n",
       " \"That's helpful\",\n",
       " 'Thanks for the help',\n",
       " 'Gracias',\n",
       " 'Ok',\n",
       " 'Perfecto',\n",
       " 'Muy bien',\n",
       " 'Who are you?',\n",
       " 'What are you?',\n",
       " 'Who you are?',\n",
       " '¿En qué puedes ayudarme?',\n",
       " '¿qué haces?',\n",
       " '¿Para qué sirves?',\n",
       " 'what is your name',\n",
       " 'what should I call you',\n",
       " '¿Quién eres?',\n",
       " '¿Cómo te llamas?',\n",
       " 'whats your name?',\n",
       " '¿Qué haces actualmente?',\n",
       " '¿Dónde estás ahora?',\n",
       " '¿Dónde trabajas?',\n",
       " 'Formación',\n",
       " 'Qué has esudiado?',\n",
       " 'Tienes estudios superiores',\n",
       " '¿Qué has hecho?',\n",
       " '¿Dónde has trabajado?',\n",
       " '¿qué empresas?',\n",
       " 'ANCERT',\n",
       " 'Agencia Notarial de Certificación',\n",
       " 'último trabajo',\n",
       " 'CSIC',\n",
       " 'Consejo Superior de Investigación Científica',\n",
       " 'trabajo anterior',\n",
       " 'JAVA',\n",
       " 'Python',\n",
       " 'SQL',\n",
       " 'PHP',\n",
       " 'VBA',\n",
       " 'bases de datos',\n",
       " 'Mysql',\n",
       " 'Spring boot',\n",
       " 'JPA',\n",
       " 'programación',\n",
       " 'desarrollo de aplicaciones',\n",
       " 'visualización',\n",
       " 'qué te gustaría hacer en el futuro?',\n",
       " 'después que termines el bootcamp',\n",
       " 'trabajo ideal?',\n",
       " 'Te interesaría tabajar con nosotros',\n",
       " 'I need to create a new account',\n",
       " 'how to open a new account',\n",
       " 'I want to create an account',\n",
       " 'can you create an account for me',\n",
       " 'how to open a new account',\n",
       " 'have a complaint',\n",
       " 'I want to raise a complaint',\n",
       " 'there is a complaint about a service']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "source": [
    "Tratamiento de los textos para alimentar la red neuronal, Sería interesante probar una lematización en la tokenización"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<OOV>': 1, 'you': 2, 'a': 3, 'qué': 4, 'to': 5, 'account': 6, '¿qué': 7, 'i': 8, 'de': 9, 'is': 10, 'are': 11, 'what': 12, 'te': 13, '¿dónde': 14, 'has': 15, 'trabajo': 16, 'create': 17, 'new': 18, 'complaint': 19, 'there': 20, 'hasta': 21, 'thanks': 22, 'for': 23, 'who': 24, 'haces': 25, 'your': 26, 'name': 27, 'el': 28, 'how': 29, 'open': 30, 'want': 31, 'an': 32, 'hi': 33, 'hey': 34, 'anyone': 35, 'hola': 36, 'eo': 37, 'hello': 38, 'yep': 39, 'tal': 40, 'buenos': 41, 'días': 42, 'buenas': 43, 'tardes': 44, 'javi': 45, 'bot': 46, 'bye': 47, 'see': 48, 'later': 49, 'goodbye': 50, 'adiós': 51, 'luego': 52, 'siempre': 53, 'thank': 54, \"that's\": 55, 'helpful': 56, 'the': 57, 'help': 58, 'gracias': 59, 'ok': 60, 'perfecto': 61, 'muy': 62, 'bien': 63, '¿en': 64, 'puedes': 65, 'ayudarme': 66, '¿para': 67, 'sirves': 68, 'should': 69, 'call': 70, '¿quién': 71, 'eres': 72, '¿cómo': 73, 'llamas': 74, 'whats': 75, 'actualmente': 76, 'estás': 77, 'ahora': 78, 'trabajas': 79, 'formación': 80, 'esudiado': 81, 'tienes': 82, 'estudios': 83, 'superiores': 84, 'hecho': 85, 'trabajado': 86, 'empresas': 87, 'ancert': 88, 'agencia': 89, 'notarial': 90, 'certificación': 91, 'último': 92, 'csic': 93, 'consejo': 94, 'superior': 95, 'investigación': 96, 'científica': 97, 'anterior': 98, 'java': 99, 'python': 100, 'sql': 101, 'php': 102, 'vba': 103, 'bases': 104, 'datos': 105, 'mysql': 106, 'spring': 107, 'boot': 108, 'jpa': 109, 'programación': 110, 'desarrollo': 111, 'aplicaciones': 112, 'visualización': 113, 'gustaría': 114, 'hacer': 115, 'en': 116, 'futuro': 117, 'después': 118, 'que': 119, 'termines': 120, 'bootcamp': 121, 'ideal': 122, 'interesaría': 123, 'tabajar': 124, 'con': 125, 'nosotros': 126, 'need': 127, 'can': 128, 'me': 129, 'have': 130, 'raise': 131, 'about': 132, 'service': 133}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000 #número de palabras del vocabulario\n",
    "embedding_dim = 16\n",
    "max_len = 20 #tamaño del vector que representará al documento\n",
    "oov_token = \"<OOV>\" #ayuda a identificar palabras que quedan fuera del vocabulario\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)#crea la bolsa de palabras\n",
    "tokenizer.fit_on_texts(training_sentences)#con las palabras de los documentos (oraciones)\n",
    "word_index = tokenizer.word_index\n",
    "#print(tokenizer.word_counts) #cuanto se repite una palabra\n",
    "#print(tokenizer.document_count) #número de docs (frases)\n",
    "print(tokenizer.word_index)#palabra, índice\n",
    "#print(tokenizer.word_docs)#palabra índice de documento\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) #transforma las frases a vectores de ínidces de palabras\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)# lleva las secuencias a vectores del mismo tamaño\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[33],\n",
       " [34],\n",
       " [10, 35, 20],\n",
       " [36],\n",
       " [37],\n",
       " [38],\n",
       " [39],\n",
       " [4, 40],\n",
       " [41, 42],\n",
       " [43, 44],\n",
       " [45, 46],\n",
       " [47],\n",
       " [48, 2, 49],\n",
       " [50],\n",
       " [51],\n",
       " [21, 52],\n",
       " [21, 53],\n",
       " [22],\n",
       " [54, 2],\n",
       " [55, 56],\n",
       " [22, 23, 57, 58],\n",
       " [59],\n",
       " [60],\n",
       " [61],\n",
       " [62, 63],\n",
       " [24, 11, 2],\n",
       " [12, 11, 2],\n",
       " [24, 2, 11],\n",
       " [64, 4, 65, 66],\n",
       " [7, 25],\n",
       " [67, 4, 68],\n",
       " [12, 10, 26, 27],\n",
       " [12, 69, 8, 70, 2],\n",
       " [71, 72],\n",
       " [73, 13, 74],\n",
       " [75, 26, 27],\n",
       " [7, 25, 76],\n",
       " [14, 77, 78],\n",
       " [14, 79],\n",
       " [80],\n",
       " [4, 15, 81],\n",
       " [82, 83, 84],\n",
       " [7, 15, 85],\n",
       " [14, 15, 86],\n",
       " [7, 87],\n",
       " [88],\n",
       " [89, 90, 9, 91],\n",
       " [92, 16],\n",
       " [93],\n",
       " [94, 95, 9, 96, 97],\n",
       " [16, 98],\n",
       " [99],\n",
       " [100],\n",
       " [101],\n",
       " [102],\n",
       " [103],\n",
       " [104, 9, 105],\n",
       " [106],\n",
       " [107, 108],\n",
       " [109],\n",
       " [110],\n",
       " [111, 9, 112],\n",
       " [113],\n",
       " [4, 13, 114, 115, 116, 28, 117],\n",
       " [118, 119, 120, 28, 121],\n",
       " [16, 122],\n",
       " [13, 123, 124, 125, 126],\n",
       " [8, 127, 5, 17, 3, 18, 6],\n",
       " [29, 5, 30, 3, 18, 6],\n",
       " [8, 31, 5, 17, 32, 6],\n",
       " [128, 2, 17, 32, 6, 23, 129],\n",
       " [29, 5, 30, 3, 18, 6],\n",
       " [130, 3, 19],\n",
       " [8, 31, 5, 131, 3, 19],\n",
       " [20, 10, 3, 19, 132, 3, 133]]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "source": [
    "Creación de la red neuronal: Secuentials"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 16)            16000     \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 16)                0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                272       \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_2 (Dense)              (None, 14)                238       \n=================================================================\nTotal params: 16,782\nTrainable params: 16,782\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6091 - accuracy: 0.8508\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6026 - accuracy: 0.8129\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6092 - accuracy: 0.8168\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6105 - accuracy: 0.8051\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5966 - accuracy: 0.8547\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6218 - accuracy: 0.8274\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6182 - accuracy: 0.8118\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5973 - accuracy: 0.8430\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6130 - accuracy: 0.8012\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5552 - accuracy: 0.8442\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6152 - accuracy: 0.8012\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5751 - accuracy: 0.8285\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6019 - accuracy: 0.8012\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6131 - accuracy: 0.8129\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5879 - accuracy: 0.8168\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6150 - accuracy: 0.7973\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5667 - accuracy: 0.8168\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5505 - accuracy: 0.8246\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5469 - accuracy: 0.8207\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5270 - accuracy: 0.8536\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5763 - accuracy: 0.8497\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5589 - accuracy: 0.8118\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5777 - accuracy: 0.8157\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5354 - accuracy: 0.8469\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5597 - accuracy: 0.8079\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.8028\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5490 - accuracy: 0.8419\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5208 - accuracy: 0.8497\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5163 - accuracy: 0.8458\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5572 - accuracy: 0.8145\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5096 - accuracy: 0.8497\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5078 - accuracy: 0.8642\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5231 - accuracy: 0.8603\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5112 - accuracy: 0.8419\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5140 - accuracy: 0.8497\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.8184\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4933 - accuracy: 0.8575\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4951 - accuracy: 0.8262\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5167 - accuracy: 0.8118\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4796 - accuracy: 0.8469\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4945 - accuracy: 0.8430\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4831 - accuracy: 0.8708\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4683 - accuracy: 0.8826\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4835 - accuracy: 0.8959\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4655 - accuracy: 0.8959\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5095 - accuracy: 0.8502\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.8658\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4618 - accuracy: 0.8747\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.8669\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8853\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8786\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4589 - accuracy: 0.8630\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4622 - accuracy: 0.8708\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.8747\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4268 - accuracy: 0.8826\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4434 - accuracy: 0.8630\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8669\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4185 - accuracy: 0.9076\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4301 - accuracy: 0.9104\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.8724\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4281 - accuracy: 0.9037\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4536 - accuracy: 0.8685\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4472 - accuracy: 0.8998\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3951 - accuracy: 0.8853\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4207 - accuracy: 0.8853\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8970\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4181 - accuracy: 0.8881\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4112 - accuracy: 0.9065\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4077 - accuracy: 0.9026\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4072 - accuracy: 0.9170\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3972 - accuracy: 0.9248\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3888 - accuracy: 0.9221\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4170 - accuracy: 0.9209\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.9131\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3928 - accuracy: 0.9327\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3863 - accuracy: 0.9315\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3850 - accuracy: 0.9354\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.9510\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.9510\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3853 - accuracy: 0.9315\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.9237\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3841 - accuracy: 0.9421\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.9343\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3979 - accuracy: 0.9343\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3679 - accuracy: 0.9499\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3694 - accuracy: 0.9198\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3534 - accuracy: 0.9510\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.9538\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.9343\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3512 - accuracy: 0.9460\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3365 - accuracy: 0.9616\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.9421\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3556 - accuracy: 0.9421\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.9616\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3304 - accuracy: 0.9605\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3521 - accuracy: 0.9527\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3242 - accuracy: 0.9527\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3158 - accuracy: 0.9644\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3474 - accuracy: 0.9448\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3121 - accuracy: 0.9644\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.9566\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3175 - accuracy: 0.9644\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3283 - accuracy: 0.9527\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.9800\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3199 - accuracy: 0.9644\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3340 - accuracy: 0.9448\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3056 - accuracy: 0.9566\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2995 - accuracy: 0.9644\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3165 - accuracy: 0.9527\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3052 - accuracy: 0.9605\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2979 - accuracy: 0.9605\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2862 - accuracy: 0.9605\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2917 - accuracy: 0.9644\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3032 - accuracy: 0.9566\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3095 - accuracy: 0.9566\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.9527\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2986 - accuracy: 0.9527\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.9644\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2878 - accuracy: 0.9632\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2701 - accuracy: 0.9710\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2883 - accuracy: 0.9894\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2591 - accuracy: 0.9894\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2904 - accuracy: 0.9816\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2730 - accuracy: 0.9816\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2797 - accuracy: 0.9933\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2801 - accuracy: 0.9789\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2579 - accuracy: 0.9789\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2676 - accuracy: 0.9789\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2670 - accuracy: 0.9710\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2847 - accuracy: 0.9632\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2676 - accuracy: 0.9710\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2573 - accuracy: 0.9710\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2712 - accuracy: 0.9710\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2482 - accuracy: 0.9749\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9749\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2731 - accuracy: 0.9632\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2636 - accuracy: 0.9632\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9710\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2414 - accuracy: 0.9828\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2587 - accuracy: 0.9632\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2511 - accuracy: 0.9749\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2487 - accuracy: 0.9828\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9828\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2459 - accuracy: 0.9749\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2196 - accuracy: 0.9789\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2323 - accuracy: 0.9749\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2334 - accuracy: 0.9749\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2231 - accuracy: 0.9894\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2306 - accuracy: 0.9894\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2308 - accuracy: 0.9632\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9710\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9789\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2038 - accuracy: 0.9789\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2171 - accuracy: 0.9789\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2278 - accuracy: 0.9710\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2152 - accuracy: 0.9828\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2121 - accuracy: 0.9789\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2154 - accuracy: 0.9749\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.9710\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2058 - accuracy: 0.9789\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2215 - accuracy: 0.9632\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9894\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1906 - accuracy: 0.9894\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9933\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9894\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9816\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1984 - accuracy: 0.9710\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2065 - accuracy: 0.9710\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9749\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2010 - accuracy: 0.9710\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2111 - accuracy: 0.9632\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9789\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2066 - accuracy: 0.9632\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9632\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.9710\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1832 - accuracy: 0.9789\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9632\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9789\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1657 - accuracy: 0.9828\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9710\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9789\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1965 - accuracy: 0.9632\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9749\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9710\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1809 - accuracy: 0.9894\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9816\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1946 - accuracy: 0.9816\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1731 - accuracy: 0.9749\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1687 - accuracy: 0.9816\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1701 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1602 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1768 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1587 - accuracy: 0.9894\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9894\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1705 - accuracy: 0.9816\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1474 - accuracy: 0.9933\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1679 - accuracy: 0.9816\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1672 - accuracy: 0.9816\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1622 - accuracy: 0.9816\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1638 - accuracy: 0.9816\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1632 - accuracy: 0.9894\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9933\n"
     ]
    }
   ],
   "source": [
    "epochs = 500 #número de veces en que todos los datos de entrenamiento pasan por la red en el proceso. Tip: aumentar el número de epoch hasta que la accuracy de los datos de validación (no los de entrenamiento) empiece a decrecer\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "source": [
    "Salvo el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: chat_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# to save the trained model\n",
    "model.save(\"chat_model\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('chat_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "with open('label_encoder.pickle', 'rb') as enc:\n",
    "    lbl_encoder = pickle.load(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../testFiles/intentsCV.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[13]\n[[3.1820178e-02 3.8853264e-11 2.4975441e-05 2.5423130e-06 3.7118504e-04\n  1.4700639e-15 3.2044053e-15 3.7280038e-02 3.9225297e-06 8.1928559e-15\n  6.2934008e-10 1.8491271e-03 2.5047932e-03 9.2614323e-01]]\n['thanks']\nChatBot: Un placer\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([\"gracias\"]),\n",
    "                                             truncating='post', maxlen=20))\n",
    "print([np.argmax(result)])\n",
    "tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "print(result)\n",
    "print(tag)\n",
    "\n",
    "for i in data['intents']:\n",
    "    if i['tag'] == tag:\n",
    "        print(\"ChatBot:\", np.random.choice(i['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}