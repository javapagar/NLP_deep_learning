{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Manejo de datos scrapeados de canciones de hip hop\n",
    "\n",
    "Estudio de los datos scrapeados de letras de canciones Hip Hop [https://www.hhgroups.com/](https://www.hhgroups.com/)\n",
    "\n",
    "Cargo los datos de un archivo pickle previamente descargado por un script de scrapping en Python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26079611"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "with open('hiphop.txt','r', encoding='utf8') as f:\n",
    "    rap_file= f.read()\n",
    "\n",
    "len(rap_file)\n",
    "\n"
   ]
  },
  {
   "source": [
    "Limpio el artista, en el texto el cantante aparece de forma '[nombre]', para eso creo una función:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25562157"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "def cleanCorchete(text):\n",
    "    return re.sub(r'(\\[.*?\\])', '', text, flags=re.DOTALL)\n",
    "\n",
    "rap_file = cleanCorchete(rap_file)\n",
    "\n",
    "len(rap_file)"
   ]
  },
  {
   "source": [
    "## Pretratamiento con spacy\n",
    "\n",
    "Descargo el lenguaje español de spacy para cnfigurarlo, ya que la mayoría del texto que voy a tratar lo utiliza"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: es-core-news-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.0.0/es_core_news_sm-3.0.0-py3-none-any.whl#egg=es_core_news_sm==3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-sm==3.0.0) (1.15.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "source": [
    "Limpio el archivo con spacy y lo guardo en un txt porque este proceso lleva tiempo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "número de batchs 26\n"
     ]
    }
   ],
   "source": [
    "#trabajo directamente con el lenguaje español de spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "#regex para limpiar los artistas entre corchetes\n",
    "rap_file=re.sub(r'(\\[.*?\\])', '', rap_file, flags=re.DOTALL)\n",
    "n =0\n",
    "start =0\n",
    "aux_clean =''\n",
    "\n",
    "for i in range(1,27):\n",
    "  n+=1\n",
    "  end=1000000 * i\n",
    "  doc = nlp(rap_file[start:end])\n",
    "  start =1000000 * i\n",
    "  filter=['-','.',',','!','?']\n",
    "  \n",
    "  for token in doc:\n",
    "\n",
    "    if token.is_alpha or token.is_digit or (token.is_punct and token.text in filter):\n",
    "\n",
    "      aux_clean+=token.text\n",
    "\n",
    "    if token.whitespace_:  \n",
    "\n",
    "      aux_clean+=token.whitespace_\n",
    "\n",
    "rap_file=aux_clean\n",
    "print(\"número de batchs\", n)\n",
    "\n",
    "with open ('hiphop_cleanv2.txt','w') as fin:\n",
    "  fin.write(rap_file)"
   ]
  },
  {
   "source": [
    "# Tratamiento de texto con tensor flow y keras\n",
    "\n",
    "## Tokenización\n",
    "\n",
    "Voy a tokenizar el texto mediante los caracteres usando keras.\n",
    "Primero cargo el fichero para crear el objeto Tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N caracteres archivo limpio: 24844054\n"
     ]
    }
   ],
   "source": [
    "with open ('hiphop_cleanv2.txt','r') as fout:\n",
    "  rap_file = fout.read()\n",
    "\n",
    "print(\"N caracteres archivo limpio:\",len(rap_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[21, 4, 8, 8, 9, 5]]\n['b a r r i o']\nTamaño del vocabulario: 67 - Tamaño del texto: 1200000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "#reduzco el texto para acelerar el entrenamiento\n",
    "text =rap_file[:1200000]\n",
    "\n",
    "#creo el toekenizer a nivel caracter y añado uno por defecto para posibles caracteres que no esten en el vocabulario (Out of vocabulary <OOV>)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "#hago una prueba de codificación y decodificación\n",
    "word_vector =tokenizer.texts_to_sequences(['Barrio'])\n",
    "print(word_vector)\n",
    "vector_word=tokenizer.sequences_to_texts(word_vector)\n",
    "print(vector_word)\n",
    "\n",
    "#número de caracteres diferentes de mi vocabulario\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "#número total de caracteres\n",
    "nChars = tokenizer.document_count\n",
    "\n",
    "print('Tamaño del vocabulario:',vocab_size, '- Tamaño del texto:', nChars)\n"
   ]
  },
  {
   "source": [
    "## Dividir el conjunto de datos secuencialmente (Train y Test)\n",
    "\n",
    "El conjunto de entrenamiento sera el 90%, 10% para test. Para alamacenar estos conjuntos de datos utilizaré el objeto Dataset de tensorflow, con vistas a manejar tensores para posteriormente pasarlo a la red neuronal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1080000, 120000)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "[codificacion] =np.array(tokenizer.texts_to_sequences([text]))-1 # la codificación irá de 0 a 78\n",
    "#con // se fuerza que el resultado sea un entero\n",
    "train_size =nChars * 90 //100\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices(codificacion[:train_size])\n",
    "test_set = tf.data.Dataset.from_tensor_slices(codificacion[train_size:])\n",
    "len(train_set),len(test_set)"
   ]
  },
  {
   "source": [
    "train_set y test_set son dos vectores con un único elemento (todo el texto con los caracteres codificaods a números), para pasarlo a una red neuronal necesito dividirlo en pequeñas porciones de texto, de  101 caracteres. El método window permite realizar esto. Comenzará en a crear ventanas de 100 elementos desde la posición uno, pasando a la dos, tres..., creando un conjuto de vectores de 101 elementos. De esta manera se divide el texto en un conjuto de ventanas de 101 caracteres.\n",
    "\n",
    "La ventana se configura con el tramaño, shift es el número de pasos que avanza la ventana cada vez y drop_remainder a True hará que el tamaño de la venatana no vaya disminuyendo al final, es decir, para que cuando queden los 101 últimos caracteres codificados el proceso se detenga."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 101\n",
    "\n",
    "train_set=train_set.repeat().window(window_size,shift=1,drop_remainder=True)\n",
    "\n",
    "test_set=test_set.repeat().window(window_size,shift=1,drop_remainder=True)"
   ]
  },
  {
   "source": [
    "Aplanamos los conjuntos de datos con el tamaño de la ventana (101), Al tener un conjunto de datos anidados, necesito aplanar esto para conseguir un conjunto de tensores con una longitud fija de 101, que se consigue fácilmente llamando a la función flat_map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set =train_set.flat_map(lambda window : window.batch(window_size))\n",
    "\n",
    "test_set =test_set.flat_map(lambda window : window.batch(window_size))"
   ]
  },
  {
   "source": [
    "Se realiza un mezclado de las ventanas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set.shuffle(10000).batch(batch_size)\n",
    "train_set = train_set.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "test_set = test_set.shuffle(10000).batch(batch_size)\n",
    "test_set = test_set.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "source": [
    "Voy a realizar la codificación one-hot para crear la bolsa de palabras con los 79 caracteres distintos que se manejaban, divido en una tupla, las secuencia que sirve para predecir y su predicción, y añado la precarga"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(lambda X, y:(tf.one_hot(X, depth=vocab_size),y))\n",
    "train_set=train_set.prefetch(1)\n",
    "\n",
    "test_set = test_set.map(lambda X, y:(tf.one_hot(X, depth=vocab_size),y))\n",
    "test_set=test_set.prefetch(1)"
   ]
  },
  {
   "source": [
    "## Creación de la red neuronal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, vocab_size],\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "Creo un par de funciones callbacks para el entrenamiento, una paraguardar el modelo por epoch y otra para parar en el caso de que no haya mejora"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb= keras.callbacks.ModelCheckpoint(\"hiphop_char_model_2/callback_model.h5\")\n",
    "stop_early = keras.callbacks.EarlyStopping(patience=3)"
   ]
  },
  {
   "source": [
    "Entreno"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set,\n",
    "                    epochs=1,\n",
    "                    validation_data = test_set,\n",
    "                    callbacks =[checkpoint_cb, stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 191674/Unknown - 62286s 325ms/step - loss: 1.6436 - accuracy: 0.4942"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-94720ba04e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m history = model.fit(train_set,\n\u001b[0m\u001b[1;32m     16\u001b[0m                     epochs=1, callbacks =[checkpoint_cb])\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "checkpoint_cb= keras.callbacks.ModelCheckpoint(\"hiphop_char_model_2/callback_model.h5\")\n",
    "\n",
    "with open('hiphop_char_model/model.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "\n",
    "model = model_from_json(json_config)\n",
    "model.load_weights('hiphop_char_model/model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "history = model.fit(train_set,\n",
    "                    epochs=1, callbacks =[checkpoint_cb])"
   ]
  },
  {
   "source": [
    "## Preprocesamiento entrada\n",
    "\n",
    "Para probar el modelo voy a crear unas funciones auxiliares que realicen el posprocesamiento: la tokenización y la codificación one-hot.\n",
    "Creo una función para predecir el siguiente caracter y otra para que cree un bucle y genere texto"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'support vector machines (e.g. the coefficients \\\\(\\\\ell_2\\\\) is the coefficients \\\\(\\\\ell_2\\\\) is th'"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "\n",
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1\n",
    "    return tf.one_hot(input_token,nDstinctChar)\n",
    "\n",
    "def next_char(input):\n",
    "    aux = treament([input])\n",
    "    predic = np.argmax(model(aux),axis=-1)\n",
    "    #predic = model(input)[0,-1:,:].numpy() +1\n",
    "    return tokenizer.sequences_to_texts(predic+1)[0][-1]\n",
    "\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=80, temperature=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text)\n",
    "    return text\n",
    "\n",
    "input = 'support vector'\n",
    "complete_text(input)"
   ]
  },
  {
   "source": [
    "Parece que el modelo, a la hora de predecir, se queda enganchado y repite la misma frase una y otra vez, es más pong lo que pongo siempre converge en el mismo discurso. Necesita introducir algo de aletoriedad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Linear model that computes the above similarity is the\\ncoefficients \\\\(\\\\ell_0\\\\) is the model '"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "def next_char(input):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / 0.8\n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "input = 'Linear model'\n",
    "complete_text(input)"
   ]
  },
  {
   "source": [
    "## Guardar el modelo de generciónde texto por caracteres"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializar el modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"char_model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serializar los pesos a HDF5\n",
    "model.save_weights(\"char_model/model.h5\")\n"
   ]
  },
  {
   "source": [
    "Mejorando el modelo anterior a través del estado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "495091"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "#Creo el dataset de train con el 90%\n",
    "train_ds_estado = tf.data.Dataset.from_tensor_slices(codificacion[:nChars * 90 // 100])\n",
    "ds_size=len(train_ds_estado)\n",
    "#creo las ventanas, esta vez no hay ni solapamiento, el númreo de ventanas se reduce\n",
    "#tampoco hay mezcla, ya que las ventanas tienen que ser secuenciales donde acaba 1 empieza otra\n",
    "train_ds_estado = train_ds_estado.window(window_size,shift =100, drop_remainder=True)\n",
    "train_ds_estado=train_ds_estado.flat_map(lambda w: w.batch(window_size))\n",
    "#los lotes serán equivalentes a la ventana, de esta manera se respetará la secuencialida entre lotes\n",
    "train_ds_estado =train_ds_estado.batch(1)\n",
    "train_ds_estado = train_ds_estado.map(lambda w: (w[:,:-1],w[:,1:]))\n",
    "train_ds_estado =train_ds_estado.map(lambda X,y: (tf.one_hot(X, depth=nDstinctChar),y))\n",
    "train_ds_estado=train_ds_estado.prefetch(1)\n",
    "ds_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, batch_input_shape= [1, None,nDstinctChar]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(nDstinctChar, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4999 - accuracy: 0.5570\n",
      "Epoch 2/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4988 - accuracy: 0.5576\n",
      "Epoch 3/50\n",
      "4950/4950 [==============================] - 233s 47ms/step - loss: 1.4989 - accuracy: 0.5578\n",
      "Epoch 4/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4967 - accuracy: 0.5582\n",
      "Epoch 5/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4967 - accuracy: 0.5581\n",
      "Epoch 6/50\n",
      "4950/4950 [==============================] - 233s 47ms/step - loss: 1.4977 - accuracy: 0.5577\n",
      "Epoch 7/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4932 - accuracy: 0.5595\n",
      "Epoch 8/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4927 - accuracy: 0.5590\n",
      "Epoch 9/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4929 - accuracy: 0.5592\n",
      "Epoch 10/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4922 - accuracy: 0.5595\n",
      "Epoch 11/50\n",
      "4950/4950 [==============================] - 234s 47ms/step - loss: 1.4922 - accuracy: 0.5601\n",
      "Epoch 12/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4904 - accuracy: 0.5609\n",
      "Epoch 13/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4909 - accuracy: 0.5598\n",
      "Epoch 14/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4930 - accuracy: 0.5589\n",
      "Epoch 15/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4920 - accuracy: 0.5591\n",
      "Epoch 16/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4866 - accuracy: 0.5611\n",
      "Epoch 17/50\n",
      "4950/4950 [==============================] - 235s 48ms/step - loss: 1.4877 - accuracy: 0.5607\n",
      "Epoch 18/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4840 - accuracy: 0.5622\n",
      "Epoch 19/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4889 - accuracy: 0.5607\n",
      "Epoch 20/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4879 - accuracy: 0.5601\n",
      "Epoch 21/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4863 - accuracy: 0.5614\n",
      "Epoch 22/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4847 - accuracy: 0.5612\n",
      "Epoch 23/50\n",
      "4950/4950 [==============================] - 235s 47ms/step - loss: 1.4870 - accuracy: 0.5616\n",
      "Epoch 24/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4865 - accuracy: 0.5612\n",
      "Epoch 25/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4851 - accuracy: 0.5611\n",
      "Epoch 26/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4853 - accuracy: 0.5618\n",
      "Epoch 27/50\n",
      "4950/4950 [==============================] - 236s 48ms/step - loss: 1.4833 - accuracy: 0.5620\n",
      "Epoch 28/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4817 - accuracy: 0.5619\n",
      "Epoch 29/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4821 - accuracy: 0.5625\n",
      "Epoch 30/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4833 - accuracy: 0.5621\n",
      "Epoch 31/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4791 - accuracy: 0.5626\n",
      "Epoch 32/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4835 - accuracy: 0.5614\n",
      "Epoch 33/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4800 - accuracy: 0.5624\n",
      "Epoch 34/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4789 - accuracy: 0.5630\n",
      "Epoch 35/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4798 - accuracy: 0.5628\n",
      "Epoch 36/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4786 - accuracy: 0.5633\n",
      "Epoch 37/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4770 - accuracy: 0.5637\n",
      "Epoch 38/50\n",
      "4950/4950 [==============================] - 237s 48ms/step - loss: 1.4792 - accuracy: 0.5626\n",
      "Epoch 39/50\n",
      "4950/4950 [==============================] - 240s 48ms/step - loss: 1.4789 - accuracy: 0.5631\n",
      "Epoch 40/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4764 - accuracy: 0.5634\n",
      "Epoch 41/50\n",
      "4950/4950 [==============================] - 238s 48ms/step - loss: 1.4786 - accuracy: 0.5626\n",
      "Epoch 42/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4796 - accuracy: 0.5625\n",
      "Epoch 43/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4747 - accuracy: 0.5645\n",
      "Epoch 44/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4726 - accuracy: 0.5647\n",
      "Epoch 45/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4740 - accuracy: 0.5644\n",
      "Epoch 46/50\n",
      "4950/4950 [==============================] - 239s 48ms/step - loss: 1.4718 - accuracy: 0.5648\n",
      "Epoch 47/50\n",
      "4950/4950 [==============================] - 240s 49ms/step - loss: 1.4703 - accuracy: 0.5654\n",
      "Epoch 48/50\n",
      "4950/4950 [==============================] - 240s 48ms/step - loss: 1.4752 - accuracy: 0.5641\n",
      "Epoch 49/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4732 - accuracy: 0.5640\n",
      "Epoch 50/50\n",
      "4950/4950 [==============================] - 241s 49ms/step - loss: 1.4738 - accuracy: 0.5644\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = ds_size // 100\n",
    "history = model.fit(train_ds_estado, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "source": [
    "El poblema del modelo anterior es que sólo permite hacer predicciones para lotes del mismo tamaño que el entrenado, por lo que la entrada debes de ser de 100 carácteres. Para poder hacer que la entrada tenga un tamaño sin determinar hay que crear una red neuranoal sin estado igual, y copiar los pesos del anterior modelo con estado entrenado a este nuevo modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_statless = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, nDstinctChar]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(nDstinctChar,activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "source": [
    "Para preparar el anterior modelo para que puedad guardar los pesos del modelo con estado habrá que especificar la estructura del tensor utilizado antes, que era la siguiente (\\[1, None,nDstinctChar\\]). Para permitir una entrada con cualquier tamaño:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_statless.build(tf.TensorShape([None,None,nDstinctChar]))\n",
    "base_model_statless.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprovecahndo el código anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'support vectors, results. with dimensionality requires a list. the cost of dorical feature ind'"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1\n",
    "    return tf.one_hot(input_token,nDstinctChar)\n",
    "\n",
    "def next_char(input,model):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / 1\n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "def complete_text(text, model,n_chars=80, temperature=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text,model)\n",
    "    return text\n",
    "\n",
    "input = 'line'\n",
    "complete_text(input, base_model_statless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializar el modelo a JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"char_model_state/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serializar los pesos a HDF5\n",
    "model.save_weights(\"char_model_state/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}