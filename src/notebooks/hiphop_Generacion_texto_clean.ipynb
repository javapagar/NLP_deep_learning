{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Manejo de datos scrapeados de canciones de hip hop\n",
    "\n",
    "Estudio de los datos scrapeados de letras de canciones Hip Hop [https://www.hhgroups.com/](https://www.hhgroups.com/)\n",
    "\n",
    "Cargo los datos de un archivo pickle previamente descargado por un script de scrapping en Python."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26079611"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "with open('hiphop.txt','r', encoding='utf8') as f:\n",
    "    rap_file= f.read()\n",
    "\n",
    "len(rap_file)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Pretratamiento con spacy\n",
    "\n",
    "Descargo el lenguaje español de spacy para cnfigurarlo, ya que la mayoría del texto que voy a tratar lo utiliza"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "source": [
    "Limpio el archivo con spacy y lo guardo en un txt porque este proceso lleva tiempo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "número de batchs 26\n"
     ]
    }
   ],
   "source": [
    "#trabajo directamente con el lenguaje español de spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "#regex para limpiar los artistas entre corchetes\n",
    "rap_file=re.sub(r'(\\[.*?\\])', '', rap_file, flags=re.DOTALL)\n",
    "n =0\n",
    "start =0\n",
    "aux_clean =''\n",
    "\n",
    "for i in range(1,27):\n",
    "  n+=1\n",
    "  end=1000000 * i\n",
    "  doc = nlp(rap_file[start:end])\n",
    "  start =1000000 * i\n",
    "  filter=['-','.',',','!','?']#estos símbolos no los borro\n",
    "  \n",
    "  for token in doc:\n",
    "\n",
    "    if token.is_alpha or token.is_digit or (token.is_punct and token.text in filter):\n",
    "\n",
    "      aux_clean+=token.text\n",
    "\n",
    "    if token.whitespace_:  \n",
    "\n",
    "      aux_clean+=token.whitespace_\n",
    "\n",
    "rap_file=aux_clean\n",
    "print(\"número de batchs\", n)\n",
    "\n",
    "with open ('hiphop_cleanv2.txt','w') as fin:\n",
    "  fin.write(rap_file)"
   ]
  },
  {
   "source": [
    "# Tratamiento de texto con tensor flow y keras\n",
    "\n",
    "## Tokenización\n",
    "\n",
    "Voy a tokenizar el texto mediante los caracteres usando keras.\n",
    "Primero cargo el fichero para crear el objeto Tokenizer\n",
    "\n",
    "## Archivo para modelo hiphop_char_model_2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N caracteres archivo limpio: 24844054\n"
     ]
    }
   ],
   "source": [
    "with open ('hiphop_cleanv2.txt','r') as fout:\n",
    "  rap_file = fout.read()\n",
    "\n",
    "print(\"N caracteres archivo limpio:\",len(rap_file))"
   ]
  },
  {
   "source": [
    "Crea el tokenizer con Tamaño del vocabulario: 67 - Tamaño del texto: 1200000."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[21, 4, 8, 8, 9, 5]]\n['b a r r i o']\nTamaño del vocabulario: 67 - Tamaño del texto: 1200000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "#reduzco el texto  a 1.200.000 caracteres para acelerar el entrenamiento\n",
    "text =rap_file[:1200000]\n",
    "\n",
    "#creo el toekenizer a nivel caracter y añado uno por defecto para posibles caracteres que no esten en el vocabulario (Out of vocabulary <OOV>)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "#hago una prueba de codificación y decodificación\n",
    "word_vector =tokenizer.texts_to_sequences(['Barrio'])\n",
    "print(word_vector)\n",
    "vector_word=tokenizer.sequences_to_texts(word_vector)\n",
    "print(vector_word)\n",
    "\n",
    "#número de caracteres diferentes de mi vocabulario\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "#número total de caracteres\n",
    "nChars = tokenizer.document_count\n",
    "\n",
    "print('Tamaño del vocabulario:',vocab_size, '- Tamaño del texto:', nChars)\n"
   ]
  },
  {
   "source": [
    "## Dividir el conjunto de datos secuencialmente (Train y Test)\n",
    "\n",
    "El conjunto de entrenamiento sera el 90%, 10% para test. Para alamacenar estos conjuntos de datos utilizaré el objeto Dataset de tensorflow, con vistas a manejar tensores para posteriormente pasarlo a la red neuronal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1080000, 120000)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "[codificacion] =np.array(tokenizer.texts_to_sequences([text]))-1 # la codificación irá de 0 a 66\n",
    "#con // se fuerza que el resultado sea un entero\n",
    "train_size =nChars * 90 //100\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices(codificacion[:train_size])\n",
    "test_set = tf.data.Dataset.from_tensor_slices(codificacion[train_size:])\n",
    "len(train_set),len(test_set)"
   ]
  },
  {
   "source": [
    "train_set y test_set son dos vectores con un único elemento (todo el texto con los caracteres codificaods a números), para pasarlo a una red neuronal necesito dividirlo en pequeñas porciones de texto, de  101 caracteres. El método window permite realizar esto. Comenzará en a crear ventanas de 100 elementos desde la posición uno, pasando a la dos, tres..., creando un conjuto de vectores de 101 elementos. De esta manera se divide el texto en un conjuto de ventanas de 101 caracteres.\n",
    "\n",
    "La ventana se configura con el tramaño, shift es el número de pasos que avanza la ventana cada vez y drop_remainder a True hará que el tamaño de la venatana no vaya disminuyendo al final, es decir, para que cuando queden los 101 últimos caracteres codificados el proceso se detenga."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 101\n",
    "\n",
    "train_set=train_set.repeat().window(window_size,shift=1,drop_remainder=True)\n",
    "\n",
    "test_set=test_set.repeat().window(window_size,shift=1,drop_remainder=True)"
   ]
  },
  {
   "source": [
    "Aplanamos los conjuntos de datos con el tamaño de la ventana (101), Al tener un conjunto de datos anidados, necesito aplanar esto para conseguir un conjunto de tensores con una longitud fija de 101, que se consigue fácilmente llamando a la función flat_map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set =train_set.flat_map(lambda window : window.batch(window_size))\n",
    "\n",
    "test_set =test_set.flat_map(lambda window : window.batch(window_size))"
   ]
  },
  {
   "source": [
    "Se realiza un mezclado de las ventanas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set.shuffle(10000).batch(batch_size)\n",
    "train_set = train_set.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "test_set = test_set.shuffle(10000).batch(batch_size)\n",
    "test_set = test_set.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "source": [
    "Voy a realizar la codificación one-hot para crear la bolsa de palabras con los 79 caracteres distintos que se manejaban, divido en una tupla, las secuencia que sirve para predecir y su predicción, y añado la precarga"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(lambda X, y:(tf.one_hot(X, depth=vocab_size),y))\n",
    "train_set=train_set.prefetch(1)\n",
    "\n",
    "test_set = test_set.map(lambda X, y:(tf.one_hot(X, depth=vocab_size),y))\n",
    "test_set=test_set.prefetch(1)"
   ]
  },
  {
   "source": [
    "## Creación de la red neuronal\n",
    "\n",
    "Creo la estructura y compilo la red neuronal modelo HipHop_char_model_2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, vocab_size],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "source": [
    "## si quiero guardar la estructura del anterior modelo: HipHop_char_model_2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"hiphop_char_model_2/model_new.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "source": [
    "# Entreno\n",
    "\n",
    "Creo un par de funciones callbacks para el entrenamiento, una paraguardar el modelo por epoch y otra para parar en el caso de que no haya mejora"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb= keras.callbacks.ModelCheckpoint(\"hiphop_char_model_2/callback_model.h5\")\n",
    "stop_early = keras.callbacks.EarlyStopping(patience=3) # las epochs son tan largas que no tienen mucho sentido"
   ]
  },
  {
   "source": [
    "El tiempo y la potencia de computación que se necesita para entrenar esta red me ha llevado a hacerlo en google colaboratory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "33750/33750 [==============================] - ETA: 0s - loss: 1.6794 - accuracy: 0.4839"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b2e298a72222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_set,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     callbacks =[checkpoint_cb, stop_early])\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1131\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_size // batch_size,\n",
    "                    #validation_data = test_set,# tengo que quitar la validación porque suma muchísimo tiempo más a la epoch\n",
    "                    callbacks =[checkpoint_cb, stop_early])\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Cargar modelos\n",
    "\n",
    "## Modelo HipHop_model_char_2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# carga la estuctura de la red\n",
    "with open('hiphop_char_model_2/model_new.json') as json_file:\n",
    "    json_config = json_file.read()\n",
    "\n",
    "model_charged = model_from_json(json_config)\n",
    "\n",
    "#carga los pesos guardados\n",
    "model_charged.load_weights('hiphop_char_model_2/model_new.h5')\n",
    "\n",
    "#Existe un error cuando la version 2.4.1 de Tensorflow es incompatible con la versión de numpy 1.20.2\n",
    "#no consigo instalar numpy 1.19.2 me da error"
   ]
  },
  {
   "source": [
    "## Preprocesamiento entrada\n",
    "\n",
    "Para probar el modelo voy a crear unas funciones auxiliares que realicen el posprocesamiento: la tokenización y la codificación one-hot.\n",
    "Creo una función para predecir el siguiente caracter y otra para que cree un bucle y genere texto"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'madre oooiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "\n",
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1 #necesita un objeto tokenizer\n",
    "    return tf.one_hot(input_token,vocab_size)\n",
    "\n",
    "def next_char(input,model):\n",
    "    aux = treament([input])\n",
    "    #char_prob = model.predict(aux)[0, -1:,:]\n",
    "    char_prob = np.argmax(model(aux),axis=1)\n",
    "    #predic = model(input)[0,-1:,:].numpy() +1\n",
    "    return tokenizer.sequences_to_texts(char_prob+1)[0][-1]\n",
    "\n",
    "\n",
    "\n",
    "def complete_text(text, model,n_chars=80, temperature=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text,model)\n",
    "    return text\n",
    "\n",
    "input = 'madre '\n",
    "complete_text(input,model_charged)"
   ]
  },
  {
   "source": [
    "Parece que el modelo, a la hora de predecir, se queda enganchado y repite la misma Letra una y otra vez, es más ponga lo que pongo siempre converge en el mismo discurso. Necesita introducir algo de aletoriedad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'heno de que la me insaré el prbecesa,el ratoe, focha. no tienen miedo0, dejo no l'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "def next_char(input,model,aletoriedad=1):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / aletoriedad \n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "input = 'h'\n",
    "complete_text(input,model_charged)"
   ]
  },
  {
   "source": [
    "## Estudio de generación de texto con aletoriedad\n",
    "\n",
    "Genero un código para que vaya generando código probando diferentes pesos de aletoriedad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9 ja déjanespin dolant dusturat,  tus cales un bed i \n",
      "upenaix  ruman.en un mang, el magursi de got que lebed,  \n",
      "dees pove sirtrie0 pi ganas pero no se pierde, como esta \n",
      "orrado, como pastala puesta el monextón sigualón el noméses  sé que siben fe\n",
      " \n",
      "0.8 jen baby, en la fe estrible, pa volver a la falma surte \n",
      "que tienen que se aprende, ah fue en esta youkbla, solo \n",
      "pare actiéndome es sus onos las suelos estás dentro con sus \n",
      "curos, ul sikerin. mi piel, me asaba el sol.mirin.bremos de goto per \n",
      " \n",
      "0.7 jos amor de ganes, esto te hago parmier, pero estás \n",
      "para mí,ahora pa si dise que los pres todos aprende  los focos \n",
      "del clumo es parey, que atís, mi altura de min hombres en \n",
      "el totel morin, faelo pero es acabo y una pracuestión.lo que te leva\n",
      " \n",
      "0.6 jo que no hay tiempo, por la tumba, si me llevo a la \n",
      "tumba,brotos de mi dian, estoy por te maso no te ha salido en \n",
      "un brozos y la pena lleva a amar el ran. en mi  entel \n",
      "de compuesta matir o a quien lo pierda y si por cuando se ecunvidé a la \n",
      " \n",
      "0.5 jo en el primero me lo siento con los bobilos de \n",
      "tiempo, sin darte que me llevo al corazón mientras me hace falta \n",
      "de mi pregunta tener que sale que se hablan la palidad. \n",
      "si me escochan a los que ma lleva a la tumba,es la oscura que los no es\n",
      " \n",
      "0.4 jos a mi por lo que no te levanta el alma y la fama de \n",
      "estar lo que me llevo a la tumba,es la pera no te he contado \n",
      "no me llevo a la tumba,es la espalda es mi sombra sin \n",
      "disparan, sobre el mill de antes de la polla y la tumba,la vida es algu\n",
      " \n",
      "0.3 jo a la puesta a la vendad la pena y la respiesta a la \n",
      "tumba, se pregunta a la muerte a la pera está bien con la \n",
      "cara, pero no es mi pared,para perder que sí, que estaba en \n",
      "el poder, mi corazón.lo que me llevo a la tumba,es lo que me llevo a\n",
      " \n",
      "0.2 jo es un marrid, como el  estaba en el poder, el \n",
      "poder.se por el poder. estaba el poder, si no estás a la \n",
      "tumba,es el poder. estoy contrando con la cara de mi perida y \n",
      "me lo pienso en un poder. si podré por mi por hombre en el poder. s\n",
      " \n",
      "0.1 jo en el poder. estoy contra de la pera erdida \n",
      "es la perdida es la pera es la pera es la pera es la pena \n",
      "      \n",
      "y la pena y la pena y la pena la pena y la pena y la pena y la pena y\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def treament(input_list):\n",
    "    input_token =np.array(tokenizer.texts_to_sequences(input_list))-1\n",
    "    return tf.one_hot(input_token,vocab_size)\n",
    "\n",
    "def next_char(input,model,aletoriedad):\n",
    "    aux = treament([input])\n",
    "    char_prob = model.predict(aux)[0, -1:,:]\n",
    "    rescaled_prob = tf.math.log(char_prob) / aletoriedad\n",
    "    char_categ = tf.random.categorical(rescaled_prob,num_samples=1)\n",
    "    return tokenizer.sequences_to_texts(char_categ.numpy() +1 )[0]\n",
    "\n",
    "\n",
    "def complete_text(text, model,n_chars=60, aletoriedad=1):\n",
    "    for i in range(n_chars):\n",
    "        text += next_char(text,model,aletoriedad)\n",
    "      \n",
    "    return text\n",
    "\n",
    "\n",
    "word = 'j'\n",
    "init=''\n",
    "rima=[]\n",
    "for i in range(1,10):\n",
    "  last = -1\n",
    "  temp =round(1 -(i /10),2)\n",
    "  for t in range(4):\n",
    "    if init == '' or len(rima)==0:\n",
    "      init=word\n",
    "    else:\n",
    "      while True:\n",
    "        #print(last)\n",
    "        \n",
    "        init =' '.join(rima[-1].split(' ')[last:])\n",
    "        if init.strip() !='' and  len(init)>4: \n",
    "          break \n",
    "        else: \n",
    "          last+=-1\n",
    "      rima[-1]=rima[-1].replace(init,'',-1)   \n",
    "    #print(init)\n",
    "    \n",
    "    \n",
    "    rima.append(complete_text(init,model_charged,aletoriedad=temp))\n",
    "\n",
    "  print (temp,'\\n'.join(rima))\n",
    "  print (' ')\n",
    "  rima=[]"
   ]
  }
 ]
}